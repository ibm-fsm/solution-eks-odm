[
  {
    "objectID": "src/key-takeaway.html",
    "href": "src/key-takeaway.html",
    "title": "Key Takeaways",
    "section": "",
    "text": "Document Status: Pre-Pilot\nThis section is currently a placeholder. Following the execution of the pilot phase, this section will be updated to reflect the actual outcomes, technical hurdles overcome, and validated best practices. The structure below outlines the key metrics and qualitative data points to be captured.\n\n\n\n\n\n\nSecurity Validation: [Pending] Confirm if OPA Gatekeeper successfully blocked non-compliant resources without impeding valid deployments.\nConnectivity: [Pending] Verify if the End-to-End TLS chain (User -&gt; ALB -&gt; Pod) functioned as expected with the self-signed internal certificates.\nPerformance: [Pending] Observations regarding the startup time of ODM pods when connecting to AWS RDS compared to local/on-prem databases.\n\n\n\n\n\nConfiguration Challenges: [Insert details on any specific Helm chart overrides or Ingress annotations that required significant debugging].\nOperational Friction: [Note any difficulties the team faced when managing secrets or certificates manually].\nUnexpected Behavior: [Document any AWS-specific quirks encountered, such as ALB provisioning delays or Security Group mapping issues].\n\n\n\n\n\nAutomation: [Plan to transition from manual secret creation to External Secrets Operator?].\nObservability: [Plan for integrating CloudWatch or Prometheus to monitor the decision runner performance?].\nScaling: [Recommendations for HPA (Horizontal Pod Autoscaler) based on the pilot load testing?].",
    "crumbs": [
      "Key Takeaways"
    ]
  },
  {
    "objectID": "src/key-takeaway.html#key-takeaways",
    "href": "src/key-takeaway.html#key-takeaways",
    "title": "Key Takeaways",
    "section": "",
    "text": "Document Status: Pre-Pilot\nThis section is currently a placeholder. Following the execution of the pilot phase, this section will be updated to reflect the actual outcomes, technical hurdles overcome, and validated best practices. The structure below outlines the key metrics and qualitative data points to be captured.\n\n\n\n\n\n\nSecurity Validation: [Pending] Confirm if OPA Gatekeeper successfully blocked non-compliant resources without impeding valid deployments.\nConnectivity: [Pending] Verify if the End-to-End TLS chain (User -&gt; ALB -&gt; Pod) functioned as expected with the self-signed internal certificates.\nPerformance: [Pending] Observations regarding the startup time of ODM pods when connecting to AWS RDS compared to local/on-prem databases.\n\n\n\n\n\nConfiguration Challenges: [Insert details on any specific Helm chart overrides or Ingress annotations that required significant debugging].\nOperational Friction: [Note any difficulties the team faced when managing secrets or certificates manually].\nUnexpected Behavior: [Document any AWS-specific quirks encountered, such as ALB provisioning delays or Security Group mapping issues].\n\n\n\n\n\nAutomation: [Plan to transition from manual secret creation to External Secrets Operator?].\nObservability: [Plan for integrating CloudWatch or Prometheus to monitor the decision runner performance?].\nScaling: [Recommendations for HPA (Horizontal Pod Autoscaler) based on the pilot load testing?].",
    "crumbs": [
      "Key Takeaways"
    ]
  },
  {
    "objectID": "src/implementation_methodology/steptwo-imp.html",
    "href": "src/implementation_methodology/steptwo-imp.html",
    "title": "ODM Deployment & Traffic Exposure",
    "section": "",
    "text": "In this final phase, we deploy the ODM workload. The configuration must bridge the gap between the Kubernetes Service (ClusterIP) and the AWS ALB, ensuring traffic remains encrypted across the boundary.\n\n\nBefore generating the deployment manifests, add the IBM Helm repository to your local client. This allows Helm to locate the ibm-odm-prod chart.\n# 1. Add the IBM Helm Repo\nhelm repo add ibm-helm https://raw.githubusercontent.com/IBM/charts/master/repo/ibm-helm\n\n# 2. Update to ensure you have the latest chart versions\nhelm repo update\n\n# 3. Verify the chart is available (Target: 25.1.0 for ODM 9.5.0.1)\nhelm search repo ibm-odm-prod\n\n\n\n\n\n\nTipReference Documentation\n\n\n\nFor a complete list of available configuration parameters, default values, and architectural details, refer to the official IBM ODM Production Helm Chart README.\n\n\n\n\n\nTo satisfy the strict OPA policies and network requirements, we must construct a specific values.yaml file. This file overrides the default “insecure” settings of the chart.\n\n\nSelect the configuration that matches your deployment phase.\n\nOption A: Production / Pilot (Target State)Option B: Lab Validation (Internal DB)\n\n\nUse Case: Deployment into the EKS Pilot environment.\nKey Features: External RDS Database, Image Digests, Strict Security Contexts.\nCreate a file named values-prod.yaml:\n# values-prod.yaml\n\n# 1. License & Auth\nlicense: true\nusersPassword: \"&lt;SET_ADMIN_PASSWORD&gt;\"\n\n# 2. Image Config (Internal Mirror)\nimage:\n  repository: artifactory.internal.corp/odm-repo\n  pullSecrets:\n    - internal-registry-secret\n  pullPolicy: Always    \n  # Note: Global tag is commented out to force component-level digests\n  # tag: \"9.5.0.1\"\n\n# 3. Component Digests (Required for 'container-image-must-have-digest' policy)\n# You must obtain the SHA256 digest from your Artifactory for each image.\ndecisionCenter:\n  tagOrDigest: \"sha256:&lt;INSERT_DC_DIGEST&gt;\"\ndecisionRunner:\n  tagOrDigest: \"sha256:&lt;INSERT_DR_DIGEST&gt;\"\ndecisionServerConsole:\n  tagOrDigest: \"sha256:&lt;INSERT_DSC_DIGEST&gt;\"\ndecisionServerRuntime:\n  tagOrDigest: \"sha256:&lt;INSERT_DSR_DIGEST&gt;\"\n\n# 4. Architecture: External Database (Required for 'psp-fsgroup' policy)\ninternalDatabase:\n  persistence:\n    enabled: false # Disable internal DB\n\nexternalDatabase:\n  type: \"postgresql\" # or \"oracle\"\n  serverName: \"&lt;RDS_ENDPOINT_ADDRESS&gt;\"\n  databaseName: \"odmdb\"\n  port: \"5432\"\n  # References the secret created in Prereqs section\n  secretCredentials: \"odm-db-secret\"\n\n# 5. Security Contexts (Native v9.5 Features)\ncustomization:\n  runAsUser: 1001\n  # NATIVE FIX: Satisfies psp-seccomp\n  seccompProfile:\n    type: RuntimeDefault\n  # NATIVE FIX: Satisfies must-have-appid\n  labels:\n    applicationid: \"ODM-PILOT\"\n\n# 6. Ingress Configuration (AWS ALB Specific)\nservice:\n  type: ClusterIP\n  enableRoute: false\n  # NATIVE FIX: Satisfies \"Host cannot be empty\" policy\n  hostname: \"odm.internal.corp\"\n\n  ingress:\n    enabled: true\n    host: \"odm.internal.corp\"\n    \n    # NOTE: When using AWS ACM, we do not use k8s TLS secrets. \n    # However, if OPA strictness requires a TLS block to be present in the YAML, \n    # leave these empty or define a dummy secret. \n    # Usually, the 'allow-http: false' annotation satisfies OPA.\n    tlsSecretRef: \"\" \n    tlsHosts: []\n\n    annotations:\n      # 1. Controller Class\n      kubernetes.io/ingress.class: alb\n      \n      # 2. Network Configuration\n      alb.ingress.kubernetes.io/scheme: internet-facing\n      # 'ip' mode routes traffic directly to Pod IPs (bypassing NodePort)\n      # This is faster and required for some sticky session configurations\n      alb.ingress.kubernetes.io/target-type: ip\n      \n      # 3. Encryption & Certificates (AWS ACM)\n      # Reference your ACM Certificate ARN here\n      alb.ingress.kubernetes.io/certificate-arn: \"arn:aws:acm:us-east-1:123456789012:certificate/xxxx-xxxx-xxxx\"\n      # Listen on HTTPS (443)\n      alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n      \n      # 4. Backend Security (Re-encryption)\n      # Tells ALB to speak HTTPS to the Pods (required for OPA compliance inside cluster)\n      alb.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n      # Ensure Health Checks also use HTTPS so pods don't fail readiness\n      alb.ingress.kubernetes.io/healthcheck-protocol: \"HTTPS\"\n      \n      # 5. OPA Compliance\n      # Explicitly disables HTTP to satisfy 'ingress-https-only' policy\n      kubernetes.io/ingress.allow-http: \"false\"\n\n\n\n\n\n\nImportantAWS ALB & OPA “TLS” Constraints\n\n\n\nStandard OPA policies (ingress-https-only) often check if the spec.tls list is populated in the Ingress YAML.\nIn AWS ALB: You typically use the certificate-arn annotation instead of a Kubernetes Secret, leaving spec.tls empty.\nIf OPA blocks this configuration: You may need to create a “dummy” self-signed secret and reference it in tlsSecretRef just to satisfy the OPA regex check, even though the ALB ignores it in favor of the ARN.\n\n\n\n\nUse Case: Sandbox testing where an External RDS is not available.\nKey Features: Internal PostgreSQL (Accepts known OPA violation for DB only), Image Digests, Self-Signed Ingress.\nCreate a file named values-lab.yaml:\n# values-lab.yaml\n\n# 1. License & Auth\nlicense: true\nusersPassword: \"odmAdminPassword123!\"\n\n# 2. Image Config (Lab Artifactory)\nimage:\n  repository: artifactory.gym.lan:8443/docker-local\n  pullSecrets:\n    - internal-registry-secret\n  # tag: \"9.5.0.1\"\n  pullPolicy: Always\n\n# 3. Component Digests (From Lab Artifactory)\ndecisionCenter:\n  tagOrDigest: \"sha256:6a0eb1f874ba52918bcd8e2c3acde2d3e428685cad7e5996e0c1227e88d3de0b\"\ndecisionRunner:\n  tagOrDigest: \"sha256:6f0643013e18d848199a73f38c5f6f854c1226ae7702c8294b835b74aa561782\"\ndecisionServerConsole:\n  tagOrDigest: \"sha256:f4c778a388535330ce5d5612d6325d5522cedb70f0cb7895fa7f015a38e5bb9c\"\ndecisionServerRuntime:\n  tagOrDigest: \"sha256:ab03e4e35923c674a090456f6869963a6d29e8f94117061ff11d383cc8c9369a\"\n\n# 4. Architecture: Internal Database\n# Note: This WILL fail 'psp-fsgroup' checks. Acceptable for Lab only.\ninternalDatabase:\n  # Digest for dbserver image\n  tagOrDigest: \"sha256:9106481ba539808ea9fed4b7d3197e91732748bc2170e862b729af8cc874f5db\"\n  persistence:\n    enabled: true\n    useDynamicProvisioning: true\n    storageClassName: \"local-path\"\n  runAsUser: 26\n\n# 5. Security Contexts\ncustomization:\n  runAsUser: 1001\n  seccompProfile:\n    type: RuntimeDefault\n  labels:\n    applicationid: \"ODM-LAB\"\n\n# 6. Ingress Configuration\nservice:\n  type: ClusterIP\n  enableRoute: false\n  hostname: \"odm.my-haproxy.gym.lan\"\n\n  ingress:\n    enabled: true\n    host: \"odm.my-haproxy.gym.lan\"\n    tlsSecretRef: \"odm-tls-secret\"\n    tlsHosts:\n      - \"odm.my-haproxy.gym.lan\"\n    annotations:\n      kubernetes.io/ingress.class: nginx\n      kubernetes.io/ingress.allow-http: \"false\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n\n\n\n\n\n\n\nWhile ODM v9.5 resolves many security configurations natively, two critical gaps remain that cannot be fixed via values.yaml alone:\n1. Group ID Enforcement: The Helm chart ignores runAsGroup and supplementalGroups for Deployments.\n2. Test Job Compliance: The odm-test-connection Job created by the chart lacks resource limits, security contexts, and image digest support.\nWe utilize Kustomize to patch these resources post-rendering.\n\n\nDefine the following files.\n\n\nTarget: Application Deployments (Decision Center, Runner, Console, Runtime).\nPurpose: Injects the mandatory Group IDs required by the “Restricted” OPA policy.\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: .*\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsUser: 1001\n        runAsGroup: 1001\n        supplementalGroups: [1001]\n\n\n\nTarget: The Database Connection Test Job.\nPurpose: This job is “unconfigurable” in the standard chart. We must patch it to enforce Image Digests, Resource Limits, and strict Security Contexts.\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: .*\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-lab-odm-test: runtime/default  \n    spec:\n      # Pod Level Security\n      securityContext:\n        runAsUser: 1001\n        runAsGroup: 1001\n        supplementalGroups: [1001]\n        seccompProfile:\n          type: RuntimeDefault\n\n      containers:\n      - name: odm-lab-odm-test\n        # CRITICAL: The Helm chart does not support digests for this specific job.\n        # You must hardcode the mirrored Runtime image and SHA256 digest here.\n        image: artifactory.internal.corp/odm-repo/odm-decisionserverruntime@sha256:&lt;INSERT_RUNTIME_SHA256_HERE&gt;\n\n        # Fix \"container-must-have-limits-and-requests\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n\n        # Fix \"privilege-escalation\", \"capabilities\", \"readonlyrootfilesystem\"\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisioncenter\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisioncenter: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-dc: runtime/default\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionrunner\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionrunner: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionrunner: runtime/default\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionserverconsole\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionserverconsole: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionserverconsole: runtime/default\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionserverruntime\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionserverruntime: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionserverruntime: runtime/default\n\n\n\n# force-pull-policy.yaml\n# Forces Main Container\n- op: replace\n  path: /spec/template/spec/containers/0/imagePullPolicy\n  value: Always\n# Forces First Init Container (e.g. init-folder-readonlyfs)\n- op: replace\n  path: /spec/template/spec/initContainers/0/imagePullPolicy\n  value: Always\n# Forces Second Init Container (e.g. init-decisionrunner)\n# Note: This will be ignored safely if a pod only has 1 init container\n- op: replace\n  path: /spec/template/spec/initContainers/1/imagePullPolicy\n  value: Always\n\n\n\n\nCreate the kustomization.yaml file to link the patches to the generated resources.\nImportant: We use explicit name targeting to ensure we do not accidentally patch the Database Deployment (if running locally) or other infrastructure components.\nNote: The names below assume your Helm Release Name is odm-lab. If you use odm-pilot, update the names accordingly (e.g., odm-pilot-odm-decisioncenter).\nresources:\n  - odm-raw.yaml\n\npatches:\n  # Generic Security Patch (Group IDs + Token)\n  # Targets ALL deployments via regex name\n  - path: security-patch.yaml\n    target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: \"odm-lab-odm-.*\"\n\n  # Specific AppArmor Patches (One per file)\n  - path: security-patch-dc.yaml\n  - path: security-patch-runner.yaml\n  - path: security-patch-console.yaml\n  - path: security-patch-runtime.yaml\n\n  # Test Job Patch\n  - path: job-security-patch.yaml\n    target:\n      group: batch\n      version: v1\n      kind: Job\n      name: odm-lab-odm-test\n\n  # Force Pull Policy (JSON Patch)\n  # Applies to all ODM deployments\n  - path: force-pull-policy.yaml\n    target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: \"odm-lab-odm-.*\"\n\n\n\n\n\n\nTipLab Workaround: Non-AppArmor Hosts (RHEL/CentOS/Rocky)\n\n\n\nIf you are running this deployment in a lab environment based on RHEL, CentOS, or Rocky Linux, your kernel likely uses SELinux instead of AppArmor.\nIncluding the container.apparmor.security.beta.kubernetes.io annotations (which are mandatory for the restricted EKS environment) will cause your lab pods to hang in a Blocked state with the error: Cannot enforce AppArmor: AppArmor is not enabled on the host.\nAction: Run the following command to strip these annotations from all YAML files in your current directory before applying the configuration:\nsed -i '/container.apparmor.security.beta.kubernetes.io/d' *.yaml\n\n\n\n\n\n\nRun the build pipeline to generate the patched manifests and apply them to the cluster.\n# 1. Render Helm Template\nhelm template odm-lab ibm-helm/ibm-odm-prod \\\n  --version 25.1.0 \\\n  --kube-version 1.28.0 \\\n  -f values-prod.yaml &gt; odm-raw.yaml\n\n# 2. Apply Patches & Deploy\nkubectl apply -k .\n\n\n\n\n\n\nTipVerification\n\n\n\nAfter deployment, verify that the ALB has successfully registered the targets.\nkubectl get ingress -n odm-pilot\n# Look for the ADDRESS field (e.g., k8s-odmpilot-xxxx.us-east-1.elb.amazonaws.com)\nNavigate to https://odm.mycompany.com/decisioncenter. If the page loads securely, End-to-End encryption is functioning correctly.",
    "crumbs": [
      "Implementation Methodology",
      "ODM Deployment"
    ]
  },
  {
    "objectID": "src/implementation_methodology/steptwo-imp.html#phase-2-odm-deployment-traffic-exposure",
    "href": "src/implementation_methodology/steptwo-imp.html#phase-2-odm-deployment-traffic-exposure",
    "title": "ODM Deployment & Traffic Exposure",
    "section": "",
    "text": "In this final phase, we deploy the ODM workload. The configuration must bridge the gap between the Kubernetes Service (ClusterIP) and the AWS ALB, ensuring traffic remains encrypted across the boundary.\n\n\nBefore generating the deployment manifests, add the IBM Helm repository to your local client. This allows Helm to locate the ibm-odm-prod chart.\n# 1. Add the IBM Helm Repo\nhelm repo add ibm-helm https://raw.githubusercontent.com/IBM/charts/master/repo/ibm-helm\n\n# 2. Update to ensure you have the latest chart versions\nhelm repo update\n\n# 3. Verify the chart is available (Target: 25.1.0 for ODM 9.5.0.1)\nhelm search repo ibm-odm-prod\n\n\n\n\n\n\nTipReference Documentation\n\n\n\nFor a complete list of available configuration parameters, default values, and architectural details, refer to the official IBM ODM Production Helm Chart README.\n\n\n\n\n\nTo satisfy the strict OPA policies and network requirements, we must construct a specific values.yaml file. This file overrides the default “insecure” settings of the chart.\n\n\nSelect the configuration that matches your deployment phase.\n\nOption A: Production / Pilot (Target State)Option B: Lab Validation (Internal DB)\n\n\nUse Case: Deployment into the EKS Pilot environment.\nKey Features: External RDS Database, Image Digests, Strict Security Contexts.\nCreate a file named values-prod.yaml:\n# values-prod.yaml\n\n# 1. License & Auth\nlicense: true\nusersPassword: \"&lt;SET_ADMIN_PASSWORD&gt;\"\n\n# 2. Image Config (Internal Mirror)\nimage:\n  repository: artifactory.internal.corp/odm-repo\n  pullSecrets:\n    - internal-registry-secret\n  pullPolicy: Always    \n  # Note: Global tag is commented out to force component-level digests\n  # tag: \"9.5.0.1\"\n\n# 3. Component Digests (Required for 'container-image-must-have-digest' policy)\n# You must obtain the SHA256 digest from your Artifactory for each image.\ndecisionCenter:\n  tagOrDigest: \"sha256:&lt;INSERT_DC_DIGEST&gt;\"\ndecisionRunner:\n  tagOrDigest: \"sha256:&lt;INSERT_DR_DIGEST&gt;\"\ndecisionServerConsole:\n  tagOrDigest: \"sha256:&lt;INSERT_DSC_DIGEST&gt;\"\ndecisionServerRuntime:\n  tagOrDigest: \"sha256:&lt;INSERT_DSR_DIGEST&gt;\"\n\n# 4. Architecture: External Database (Required for 'psp-fsgroup' policy)\ninternalDatabase:\n  persistence:\n    enabled: false # Disable internal DB\n\nexternalDatabase:\n  type: \"postgresql\" # or \"oracle\"\n  serverName: \"&lt;RDS_ENDPOINT_ADDRESS&gt;\"\n  databaseName: \"odmdb\"\n  port: \"5432\"\n  # References the secret created in Prereqs section\n  secretCredentials: \"odm-db-secret\"\n\n# 5. Security Contexts (Native v9.5 Features)\ncustomization:\n  runAsUser: 1001\n  # NATIVE FIX: Satisfies psp-seccomp\n  seccompProfile:\n    type: RuntimeDefault\n  # NATIVE FIX: Satisfies must-have-appid\n  labels:\n    applicationid: \"ODM-PILOT\"\n\n# 6. Ingress Configuration (AWS ALB Specific)\nservice:\n  type: ClusterIP\n  enableRoute: false\n  # NATIVE FIX: Satisfies \"Host cannot be empty\" policy\n  hostname: \"odm.internal.corp\"\n\n  ingress:\n    enabled: true\n    host: \"odm.internal.corp\"\n    \n    # NOTE: When using AWS ACM, we do not use k8s TLS secrets. \n    # However, if OPA strictness requires a TLS block to be present in the YAML, \n    # leave these empty or define a dummy secret. \n    # Usually, the 'allow-http: false' annotation satisfies OPA.\n    tlsSecretRef: \"\" \n    tlsHosts: []\n\n    annotations:\n      # 1. Controller Class\n      kubernetes.io/ingress.class: alb\n      \n      # 2. Network Configuration\n      alb.ingress.kubernetes.io/scheme: internet-facing\n      # 'ip' mode routes traffic directly to Pod IPs (bypassing NodePort)\n      # This is faster and required for some sticky session configurations\n      alb.ingress.kubernetes.io/target-type: ip\n      \n      # 3. Encryption & Certificates (AWS ACM)\n      # Reference your ACM Certificate ARN here\n      alb.ingress.kubernetes.io/certificate-arn: \"arn:aws:acm:us-east-1:123456789012:certificate/xxxx-xxxx-xxxx\"\n      # Listen on HTTPS (443)\n      alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n      \n      # 4. Backend Security (Re-encryption)\n      # Tells ALB to speak HTTPS to the Pods (required for OPA compliance inside cluster)\n      alb.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n      # Ensure Health Checks also use HTTPS so pods don't fail readiness\n      alb.ingress.kubernetes.io/healthcheck-protocol: \"HTTPS\"\n      \n      # 5. OPA Compliance\n      # Explicitly disables HTTP to satisfy 'ingress-https-only' policy\n      kubernetes.io/ingress.allow-http: \"false\"\n\n\n\n\n\n\nImportantAWS ALB & OPA “TLS” Constraints\n\n\n\nStandard OPA policies (ingress-https-only) often check if the spec.tls list is populated in the Ingress YAML.\nIn AWS ALB: You typically use the certificate-arn annotation instead of a Kubernetes Secret, leaving spec.tls empty.\nIf OPA blocks this configuration: You may need to create a “dummy” self-signed secret and reference it in tlsSecretRef just to satisfy the OPA regex check, even though the ALB ignores it in favor of the ARN.\n\n\n\n\nUse Case: Sandbox testing where an External RDS is not available.\nKey Features: Internal PostgreSQL (Accepts known OPA violation for DB only), Image Digests, Self-Signed Ingress.\nCreate a file named values-lab.yaml:\n# values-lab.yaml\n\n# 1. License & Auth\nlicense: true\nusersPassword: \"odmAdminPassword123!\"\n\n# 2. Image Config (Lab Artifactory)\nimage:\n  repository: artifactory.gym.lan:8443/docker-local\n  pullSecrets:\n    - internal-registry-secret\n  # tag: \"9.5.0.1\"\n  pullPolicy: Always\n\n# 3. Component Digests (From Lab Artifactory)\ndecisionCenter:\n  tagOrDigest: \"sha256:6a0eb1f874ba52918bcd8e2c3acde2d3e428685cad7e5996e0c1227e88d3de0b\"\ndecisionRunner:\n  tagOrDigest: \"sha256:6f0643013e18d848199a73f38c5f6f854c1226ae7702c8294b835b74aa561782\"\ndecisionServerConsole:\n  tagOrDigest: \"sha256:f4c778a388535330ce5d5612d6325d5522cedb70f0cb7895fa7f015a38e5bb9c\"\ndecisionServerRuntime:\n  tagOrDigest: \"sha256:ab03e4e35923c674a090456f6869963a6d29e8f94117061ff11d383cc8c9369a\"\n\n# 4. Architecture: Internal Database\n# Note: This WILL fail 'psp-fsgroup' checks. Acceptable for Lab only.\ninternalDatabase:\n  # Digest for dbserver image\n  tagOrDigest: \"sha256:9106481ba539808ea9fed4b7d3197e91732748bc2170e862b729af8cc874f5db\"\n  persistence:\n    enabled: true\n    useDynamicProvisioning: true\n    storageClassName: \"local-path\"\n  runAsUser: 26\n\n# 5. Security Contexts\ncustomization:\n  runAsUser: 1001\n  seccompProfile:\n    type: RuntimeDefault\n  labels:\n    applicationid: \"ODM-LAB\"\n\n# 6. Ingress Configuration\nservice:\n  type: ClusterIP\n  enableRoute: false\n  hostname: \"odm.my-haproxy.gym.lan\"\n\n  ingress:\n    enabled: true\n    host: \"odm.my-haproxy.gym.lan\"\n    tlsSecretRef: \"odm-tls-secret\"\n    tlsHosts:\n      - \"odm.my-haproxy.gym.lan\"\n    annotations:\n      kubernetes.io/ingress.class: nginx\n      kubernetes.io/ingress.allow-http: \"false\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n\n\n\n\n\n\n\nWhile ODM v9.5 resolves many security configurations natively, two critical gaps remain that cannot be fixed via values.yaml alone:\n1. Group ID Enforcement: The Helm chart ignores runAsGroup and supplementalGroups for Deployments.\n2. Test Job Compliance: The odm-test-connection Job created by the chart lacks resource limits, security contexts, and image digest support.\nWe utilize Kustomize to patch these resources post-rendering.\n\n\nDefine the following files.\n\n\nTarget: Application Deployments (Decision Center, Runner, Console, Runtime).\nPurpose: Injects the mandatory Group IDs required by the “Restricted” OPA policy.\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: .*\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsUser: 1001\n        runAsGroup: 1001\n        supplementalGroups: [1001]\n\n\n\nTarget: The Database Connection Test Job.\nPurpose: This job is “unconfigurable” in the standard chart. We must patch it to enforce Image Digests, Resource Limits, and strict Security Contexts.\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: .*\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-lab-odm-test: runtime/default  \n    spec:\n      # Pod Level Security\n      securityContext:\n        runAsUser: 1001\n        runAsGroup: 1001\n        supplementalGroups: [1001]\n        seccompProfile:\n          type: RuntimeDefault\n\n      containers:\n      - name: odm-lab-odm-test\n        # CRITICAL: The Helm chart does not support digests for this specific job.\n        # You must hardcode the mirrored Runtime image and SHA256 digest here.\n        image: artifactory.internal.corp/odm-repo/odm-decisionserverruntime@sha256:&lt;INSERT_RUNTIME_SHA256_HERE&gt;\n\n        # Fix \"container-must-have-limits-and-requests\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n\n        # Fix \"privilege-escalation\", \"capabilities\", \"readonlyrootfilesystem\"\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisioncenter\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisioncenter: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-dc: runtime/default\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionrunner\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionrunner: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionrunner: runtime/default\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionserverconsole\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionserverconsole: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionserverconsole: runtime/default\n\n\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionserverruntime\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionserverruntime: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionserverruntime: runtime/default\n\n\n\n# force-pull-policy.yaml\n# Forces Main Container\n- op: replace\n  path: /spec/template/spec/containers/0/imagePullPolicy\n  value: Always\n# Forces First Init Container (e.g. init-folder-readonlyfs)\n- op: replace\n  path: /spec/template/spec/initContainers/0/imagePullPolicy\n  value: Always\n# Forces Second Init Container (e.g. init-decisionrunner)\n# Note: This will be ignored safely if a pod only has 1 init container\n- op: replace\n  path: /spec/template/spec/initContainers/1/imagePullPolicy\n  value: Always\n\n\n\n\nCreate the kustomization.yaml file to link the patches to the generated resources.\nImportant: We use explicit name targeting to ensure we do not accidentally patch the Database Deployment (if running locally) or other infrastructure components.\nNote: The names below assume your Helm Release Name is odm-lab. If you use odm-pilot, update the names accordingly (e.g., odm-pilot-odm-decisioncenter).\nresources:\n  - odm-raw.yaml\n\npatches:\n  # Generic Security Patch (Group IDs + Token)\n  # Targets ALL deployments via regex name\n  - path: security-patch.yaml\n    target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: \"odm-lab-odm-.*\"\n\n  # Specific AppArmor Patches (One per file)\n  - path: security-patch-dc.yaml\n  - path: security-patch-runner.yaml\n  - path: security-patch-console.yaml\n  - path: security-patch-runtime.yaml\n\n  # Test Job Patch\n  - path: job-security-patch.yaml\n    target:\n      group: batch\n      version: v1\n      kind: Job\n      name: odm-lab-odm-test\n\n  # Force Pull Policy (JSON Patch)\n  # Applies to all ODM deployments\n  - path: force-pull-policy.yaml\n    target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: \"odm-lab-odm-.*\"\n\n\n\n\n\n\nTipLab Workaround: Non-AppArmor Hosts (RHEL/CentOS/Rocky)\n\n\n\nIf you are running this deployment in a lab environment based on RHEL, CentOS, or Rocky Linux, your kernel likely uses SELinux instead of AppArmor.\nIncluding the container.apparmor.security.beta.kubernetes.io annotations (which are mandatory for the restricted EKS environment) will cause your lab pods to hang in a Blocked state with the error: Cannot enforce AppArmor: AppArmor is not enabled on the host.\nAction: Run the following command to strip these annotations from all YAML files in your current directory before applying the configuration:\nsed -i '/container.apparmor.security.beta.kubernetes.io/d' *.yaml\n\n\n\n\n\n\nRun the build pipeline to generate the patched manifests and apply them to the cluster.\n# 1. Render Helm Template\nhelm template odm-lab ibm-helm/ibm-odm-prod \\\n  --version 25.1.0 \\\n  --kube-version 1.28.0 \\\n  -f values-prod.yaml &gt; odm-raw.yaml\n\n# 2. Apply Patches & Deploy\nkubectl apply -k .\n\n\n\n\n\n\nTipVerification\n\n\n\nAfter deployment, verify that the ALB has successfully registered the targets.\nkubectl get ingress -n odm-pilot\n# Look for the ADDRESS field (e.g., k8s-odmpilot-xxxx.us-east-1.elb.amazonaws.com)\nNavigate to https://odm.mycompany.com/decisioncenter. If the page loads securely, End-to-End encryption is functioning correctly.",
    "crumbs": [
      "Implementation Methodology",
      "ODM Deployment"
    ]
  },
  {
    "objectID": "src/landing_page/landing_page.html",
    "href": "src/landing_page/landing_page.html",
    "title": "Project Name",
    "section": "",
    "text": "Project Name\nSubtitle\n\n\nOur Documentation \n\n\n\n\n\nNext Steps\n\n\n\nL ink 1\n\n\n\nLink 2"
  },
  {
    "objectID": "src/solution_overview/prepare.html",
    "href": "src/solution_overview/prepare.html",
    "title": "Deployment Readiness",
    "section": "",
    "text": "Successful deployment of IBM ODM into a strictly governed EKS environment requires specific infrastructure pillars to be established prior to installation. This section outlines the necessary tooling, external services, and cluster configurations required to satisfy the Restricted OPA Gatekeeper policies.",
    "crumbs": [
      "Solution Overview",
      "Prepare"
    ]
  },
  {
    "objectID": "src/solution_overview/prepare.html#prerequisite-checklist",
    "href": "src/solution_overview/prepare.html#prerequisite-checklist",
    "title": "Deployment Readiness",
    "section": "Prerequisite Checklist",
    "text": "Prerequisite Checklist\nBefore initiating the deployment pipeline, ensure the following prerequisites are met.\n\n1. Workstation & Tooling\nThe automation bastion or engineer’s workstation requires connectivity to the target EKS cluster and the following CLI tools:\n\nHelm 3 (v3.10+ recommended) for chart management.\nKubectl configured with the correct context.\nKustomize (v4+ or built-in via kubectl -k) for manifest post-rendering.\nOpenShift CLI (oc) or Skopeo (Optional): Recommended tools for manual image mirroring if an automated enterprise pipeline is not available.\n\n\n\n2. Database Strategy\nThe standard containerized database included with the product requires privileged filesystem groups (fsGroup: 26), which is incompatible with the target environment’s OPA policies.\nPlease select ONE of the following compliant strategies for this deployment:\nOption A: AWS RDS (Production Recommended) Utilize a managed database service to completely offload persistence management and security compliance from the Kubernetes cluster.\n\nRequirement: Provision AWS RDS (PostgreSQL 12+ or Oracle).\n\nConfiguration: Ensure the database is accessible from the EKS Node Security Group.\n\nCredentials: Obtain the Endpoint URL, Port, Database Name, Username, and Password.\n\nOption B: Compliant Internal Container (Pilot/PoC Alternative) For non-production pilot environments, the client may provide their own approved PostgreSQL container image to run within the same namespace.\n\nRequirement: The container image must be security-hardened and configured to run as a non-root user (UID &gt; 1001) to pass OPA checks.\n\nConfiguration: The client is responsible for defining the deployment and storage resources for this custom database container.\n\n\n\n3. Supply Chain (Image Mirroring)\nThe target environment prohibits direct access to public registries (cp.icr.io). All container images must be staged in the client’s internal trusted registry (e.g., Artifactory).\nEnterprise Pipeline Integration If the organization utilizes a centralized image ingestion process, please configure the pipeline to pull from the IBM Entitled Registry using the source details below.\n\nSource Registry: cp.icr.io\nSource Namespace: cp/cp4a/odm\nTag: 9.5.0.1\n\nRequired Images:\n\n\n\nComponent\nImage Name\n\n\n\n\nDecision Center\nodm-decisioncenter\n\n\nDecision Runner\nodm-decisionrunner\n\n\nDecision Server Console\nodm-decisionserverconsole\n\n\nDecision Server Runtime\nodm-decisionserverruntime\n\n\nDatabase Init Utility\ndbserver\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nManual Mirroring Alternative: In the absence of an automated ingestion pipeline, we recommend using the OpenShift CLI (oc) or Skopeo. These tools efficiently copy multi-architecture manifest lists between registries without requiring intermediate disk storage or Docker daemons.\n\n\n\n\n4. Cluster Configuration\nThe Kubernetes namespace must be prepared with the necessary secrets and networking definitions.\n\nNamespace: Create a dedicated namespace (e.g., odm-pilot).\nTLS Secret: A pre-provisioned Kubernetes Secret (type kubernetes.io/tls) containing the valid certificate and private key for the Ingress controller.\n\nRequirement: The certificate Subject Alternative Name (SAN) must match the intended Ingress host (e.g., odm.internal.corp).\nNote: In a production environment, this secret is typically provisioned by the organization’s PKI automation (e.g., Cert-Manager or Venafi). For Lab/Pilot implementation steps, see the Implementation Methodology section.\n\nStorage Class (Conditional):\n\nIf using Option B (Internal Container): A Storage Class must be identified to provision the Persistent Volume for the database.\nIf using Option A (AWS RDS): No Storage Class is required for the database layer.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOPA Policy Alignment: Ensure that the Namespace does not have any legacy PodSecurityPolicies attached that might conflict with the OPA Gatekeeper constraints. The solution relies entirely on the OPA constraints for security governance.\n\n\n\n\n\n\n\n\nTip\n\n\n\nLab & Sandbox Setup: If you are attempting to reproduce this environment in a local lab (e.g., k3s or Minikube) and need to manually install OPA Gatekeeper to simulate the constraint layer, please refer to the Lab Setup Guide: OPA Gatekeeper Configuration.",
    "crumbs": [
      "Solution Overview",
      "Prepare"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Secure Deployment of IBM ODM on AWS EKS",
    "section": "",
    "text": "SL",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "index.html#description-secure-deployment-of-ibm-odm-on-aws-eks",
    "href": "index.html#description-secure-deployment-of-ibm-odm-on-aws-eks",
    "title": "Secure Deployment of IBM ODM on AWS EKS",
    "section": "Description: “Secure Deployment of IBM ODM on AWS EKS”",
    "text": "Description: “Secure Deployment of IBM ODM on AWS EKS”",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "index.html#the-why",
    "href": "index.html#the-why",
    "title": "Secure Deployment of IBM ODM on AWS EKS",
    "section": "The Why",
    "text": "The Why\nThe client is executing a strategic initiative to modernize their business rule management systems, transitioning critical decision logic from the mainframe to a cloud-native architecture on AWS EKS. This modernization effort requires the agility of containerization without compromising the rigorous security standards inherent to the financial services sector.\n\nBalancing Agility and Security: The primary challenge lies in reconciling the rapid deployment capabilities of IBM ODM with the client’s “Zero Exception” security posture.\nStrict Compliance alignment: Financial services environments often enforce “Restricted” Pod Security Standards that exceed standard Kubernetes defaults. Deploying complex enterprise software into these environments requires bridging the gap between standard deployment manifests and bespoke security constraints.\nOperational Standardization: The client requires a repeatable, automated deployment pipeline that satisfies automated policy gates (OPA Gatekeeper) without requiring manual intervention or policy waivers.\n\n\nProblem Details\nTechnical Hurdles & Policy Integration\nThe objective is to deploy IBM Operational Decision Manager (ODM) v9.5 into an AWS EKS cluster protected by OPA Gatekeeper constraints. While the IBM ODM Helm Chart covers the vast majority of Kubernetes security configurations, this specific environment enforces a distinct set of “Restricted” policies that require granular control over every aspect of the Pod Security Context.\nThe specific integration challenges addressed in this solution include:\n\nGranular Security Context Enforcement: The target environment mandates explicit definitions for runAsGroup and supplementalGroups at the Pod level. The solution requires a method to inject these specific, client-mandated security contexts into the standard deployment manifests during the CI/CD process.\nZero-Trust Persistence Architecture: The standard internal database configuration utilizes specific filesystem groups (fsGroup) for storage permission management. To align with the client’s non-root requirements (UID/GID &gt; 1000), the architecture must be adapted to utilize an External Database (e.g. AWS RDS) rather than containerized persistence.\nSecure Supply Chain: The environment prohibits pulling images from public or vendor registries. The deployment workflow must accommodate an air-gapped supply chain, utilizing an internal artifact repository and referencing all container images strictly by SHA256 digests.\nIngress Hardening: To meet strict traffic control policies, the solution must explicitly disable HTTP pathways and enforce TLS termination and specific ingress host matching at the manifest level.\n\n\n\nAdditional Context\nEnvironment & Constraints\n\nPlatform: AWS Elastic Kubernetes Service (EKS).\nPolicy Engine: Open Policy Agent (OPA) Gatekeeper enforcing the Kubernetes “Restricted” Pod Security Standard.\nConstraint Strictness:\n\nImmutable Policies: The platform team does not allow policy exceptions or namespace-level whitelisting.\nPre-Deployment Validation: Deployment manifests must be fully compliant before applying to the cluster; relying on post-admission mutations is not permitted.\n\nSoftware Version: IBM ODM 9.5.0.1 (Helm Chart 25.1.0).\nNetwork Posture: Strict egress filtering requires explicit allowance for connectivity to external services (RDS) and internal tooling (Artifactory).",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "src/solution_overview/environment.html",
    "href": "src/solution_overview/environment.html",
    "title": "Environment Architecture",
    "section": "",
    "text": "This section details the reference architecture for the IBM ODM v9.5 deployment. The environment is designed to operate within a “Restricted” Kubernetes security context, utilizing externalized persistence and strict traffic controls to satisfy organizational compliance standards.",
    "crumbs": [
      "Solution Overview",
      "Environment"
    ]
  },
  {
    "objectID": "src/solution_overview/environment.html#logical-topology",
    "href": "src/solution_overview/environment.html#logical-topology",
    "title": "Environment Architecture",
    "section": "Logical Topology",
    "text": "Logical Topology\nThe deployment architecture isolates the ODM application logic from the persistence layer, ensuring that all compute resources within the Kubernetes cluster remain stateless and ephemeral. The solution utilizes a single-namespace model where application pods connect securely to externalized infrastructure services.\n\nCompute: Stateless Pods running on AWS EKS Worker Nodes.\nPersistence: External AWS RDS (PostgreSQL) for transactional data.\nArtifacts: Internal Trusted Registry (Artifactory) for container images.\nRouting: Ingress Controller handling TLS termination and routing to internal ClusterIP services.\n\n\n\n\n\n\ngraph LR\n    %% ---------------------------------------------------------\n    %% STYLING DEFINITIONS\n    %% ---------------------------------------------------------\n    classDef darkBlue fill:#002C6D,stroke:#333,stroke-width:2px,color:white;\n    classDef lightBlue fill:#6BA2C1,stroke:#333,stroke-width:2px,color:white;\n    classDef green fill:#368727,stroke:#333,stroke-width:2px,color:white;\n    classDef white fill:#ffffff,stroke:#333,stroke-width:1px,color:black;\n    %% Invisible style for spacing wrappers\n    classDef invisible fill:none,stroke:none,color:none;\n\n    %% ---------------------------------------------------------\n    %% NODE DEFINITIONS\n    %% ---------------------------------------------------------\n    subgraph CN [\"Corporate Network\"]\n        User((\"User\"))\n        Admin((\"Admin\"))\n    end\n\n    subgraph AWS_CLOUD [\"AWS Cloud Environment\"]\n        %% SPACER 1: Pushes content away from 'AWS Cloud Environment' title\n        subgraph CLOUD_SPACER [\" \"]\n            direction LR\n            \n            ALB[\"AWS ALB&lt;br/&gt;(Terminates & Re-encrypts)\"]\n\n            subgraph EKS [\"AWS EKS Cluster\"]\n                %% SPACER 2: Pushes content away from 'AWS EKS Cluster' title\n                subgraph EKS_SPACER [\" \"]\n                    direction LR\n                    \n                    subgraph NS [\"Namespace: odm-pilot\"]\n                        %% SPACER 3: Pushes content away from 'Namespace' title\n                        subgraph NS_SPACER [\" \"]\n                            \n                            subgraph ODM [\"ODM Workload (UID 1001)\"]\n                                DC[\"Decision Center\"]\n                                DR[\"Decision Runner\"]\n                                DSC[\"DS Console\"]\n                                DSR[\"DS Runtime\"]\n                            end\n                        end\n                    end\n                end\n            end\n\n            Database[(\"AWS RDS PostgreSQL\")]\n        end\n    end\n\n    subgraph EXT [\"External Infrastructure\"]\n        Registry[(\"Internal Artifactory\")]\n    end\n\n    %% ---------------------------------------------------------\n    %% CONNECTIONS\n    %% ---------------------------------------------------------\n    \n    %% Inbound HTTPS\n    User --&gt;|\"HTTPS\"| ALB\n    Admin --&gt;|\"HTTPS\"| ALB\n    \n    %% Internal Re-encrypted HTTPS\n    ALB --&gt;|\"HTTPS\"| DC\n    ALB --&gt;|\"HTTPS\"| DR\n    ALB --&gt;|\"HTTPS\"| DSC\n    ALB --&gt;|\"HTTPS\"| DSR\n\n    %% Database Connectivity\n    DC --&gt;|\"JDBC/TCP 5432\"| Database\n    DR --&gt;|\"JDBC/TCP 5432\"| Database\n    DSC --&gt;|\"JDBC/TCP 5432\"| Database\n    DSR --&gt;|\"JDBC/TCP 5432\"| Database\n\n    %% Image Pulls\n    DC -.-&gt;|\"Image Pull\"| Registry\n    DR -.-&gt;|\"Image Pull\"| Registry\n    DSC -.-&gt;|\"Image Pull\"| Registry\n    DSR -.-&gt;|\"Image Pull\"| Registry\n\n    %% ---------------------------------------------------------\n    %% APPLY STYLES\n    %% ---------------------------------------------------------\n    class User,Admin darkBlue;\n    class DC,DR,DSC,DSR lightBlue;\n    class Database,ALB green;\n    class Registry white;\n    \n    %% Apply invisible style to all spacer subgraphs\n    class CLOUD_SPACER,EKS_SPACER,NS_SPACER invisible;",
    "crumbs": [
      "Solution Overview",
      "Environment"
    ]
  },
  {
    "objectID": "src/solution_overview/environment.html#component-matrix",
    "href": "src/solution_overview/environment.html#component-matrix",
    "title": "Environment Architecture",
    "section": "Component Matrix",
    "text": "Component Matrix\nThe solution validates the integration of the following specific software versions.\n\n\n\n\n\n\n\n\nComponent\nVersion\nRole\n\n\n\n\nPlatform\nAWS EKS (K8s 1.24+)\nContainer Orchestration Platform\n\n\nSoftware\nIBM ODM 9.5.0.1\nBusiness Rule Management System\n\n\nHelm Chart\nibm-odm-prod 25.1.0\nDeployment Manager\n\n\nDatabase\nPostgreSQL 12+\nExternal Persistence Layer (AWS RDS)\n\n\nPolicy Engine\nOPA Gatekeeper\nSecurity Governance & Admission Control\n\n\nIngress\nNGINX / AWS ALB\nTraffic Routing & TLS Termination",
    "crumbs": [
      "Solution Overview",
      "Environment"
    ]
  },
  {
    "objectID": "src/solution_overview/environment.html#security-context-specification",
    "href": "src/solution_overview/environment.html#security-context-specification",
    "title": "Environment Architecture",
    "section": "Security Context Specification",
    "text": "Security Context Specification\nTo comply with the Restricted Pod Security Standards enforced by OPA Gatekeeper, the ODM application containers are configured with a strict security profile. This profile overrides standard defaults to ensure “Zero Privilege” execution.\n\nPod Security Settings\nThe deployment pipeline explicitly injects the following contexts into all workload resources:\n\nUser ID (UID): 1001 (Non-Root)\nGroup ID (GID): 1001 (Non-Root)\nFilesystem: Read-Only Root Filesystem (with specific volume mounts for temp directories)\nPrivilege Escalation: AllowPrivilegeEscalation: false\nCapabilities: DROP ALL\nSeccomp Profile: RuntimeDefault\n\n\n\n\n\n\n\nNote\n\n\n\nService Account Token: To minimize the attack surface, automountServiceAccountToken is disabled on application pods. This configuration is validated for core ODM functionality, though it restricts the usage of the standard IBM License Metering agent sidecar.",
    "crumbs": [
      "Solution Overview",
      "Environment"
    ]
  },
  {
    "objectID": "src/solution_overview/environment.html#network-connectivity",
    "href": "src/solution_overview/environment.html#network-connectivity",
    "title": "Environment Architecture",
    "section": "Network & Connectivity",
    "text": "Network & Connectivity\nThe environment assumes a “Deny by Default” network posture.\n\nIngress (Inbound)\n\nProtocol: HTTPS Only (HTTP traffic is strictly disabled at the Ingress level).\nTermination: TLS is terminated at the Ingress Controller using a Kubernetes Secret.\nRouting: Traffic is routed to internal ClusterIP services. No NodePorts or LoadBalancers are created directly by the application.\n\n\n\nEgress (Outbound)\nThe ODM Pods require outbound network access to the following destinations:\n\nDatabase: TCP access to the AWS RDS endpoint (typically port 5432).\nImage Registry: HTTPS access to the internal Artifactory for image pulling.\nInternal DNS: UDP/TCP access to the cluster CoreDNS service.",
    "crumbs": [
      "Solution Overview",
      "Environment"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html",
    "href": "src/solution_overview/troubleshooting.html",
    "title": "Troubleshooting & Diagnostics",
    "section": "",
    "text": "This section outlines common issues encountered during the deployment of Operational Decision Manager (ODM) on Amazon EKS. It focuses specifically on the challenges introduced by the strict security requirements: end-to-end TLS encryption (HTTPS everywhere) and restrictive database network policies.",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#overview",
    "href": "src/solution_overview/troubleshooting.html#overview",
    "title": "Troubleshooting & Diagnostics",
    "section": "",
    "text": "This section outlines common issues encountered during the deployment of Operational Decision Manager (ODM) on Amazon EKS. It focuses specifically on the challenges introduced by the strict security requirements: end-to-end TLS encryption (HTTPS everywhere) and restrictive database network policies.",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#logging-strategy",
    "href": "src/solution_overview/troubleshooting.html#logging-strategy",
    "title": "Troubleshooting & Diagnostics",
    "section": "Logging Strategy",
    "text": "Logging Strategy\nEffective troubleshooting requires inspecting logs at three distinct layers. Use the following commands to isolate errors.\n\nODM Application Logs\nThe most critical logs are generated by the WebSphere Liberty server running inside the ODM pods. Look here for JDBC connection errors, rule execution failures, or startup timeouts.\n# Stream logs for the Decision Center pod\nkubectl logs -f -l app=odm-decision-center -n odm-pilot\n\n# Check for specific \"messages.log\" errors inside a running container\nkubectl exec -it &lt;pod-name&gt; -n odm-pilot -- cat /logs/messages.log\n\n\nAWS Load Balancer Controller Logs\nIf the Ingress resource is created but the AWS ALB or Target Groups are not provisioning, inspect the controller logs in the kube-system namespace.\nkubectl logs -f -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller\n\n\nKubernetes Events\nUse events to diagnose scheduling issues, resource quota limits, or image pull failures.\nkubectl get events -n odm-pilot --sort-by='.lastTimestamp'",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#common-failure-scenarios",
    "href": "src/solution_overview/troubleshooting.html#common-failure-scenarios",
    "title": "Troubleshooting & Diagnostics",
    "section": "Common Failure Scenarios",
    "text": "Common Failure Scenarios\nUse this matrix to quickly resolve the most frequent deployment errors.\n\n\n\n\n\n\n\n\nSymptom\nProbable Cause\nCorrective Action\n\n\n\n\nALB returns 502 Bad Gateway\nProtocol Mismatch. The ALB is sending HTTP to a Pod expecting HTTPS, or the Pod is failing its readiness probe.\n1. Verify the Ingress annotation exists: alb.ingress.kubernetes.io/backend-protocol: HTTPS2. Check if the Pod’s Readiness Probe is failing (see Section 1.3).\n\n\nPod stuck in CrashLoopBackOff\nDatabase Failure. The ODM container cannot reach RDS to initialize the schema.\n1. Check pod logs for Connection refused or SocketTimeout.2. Verify the RDS Security Group allows inbound traffic on port 5432 from the EKS Node Security Group.\n\n\nDeployment fails OPA Validation\nPolicy Violation. The deployment spec exposes a non-secure port (HTTP).\nEnsure the container spec creates the service on port 9443 (HTTPS) rather than 9060. The OPA policy mandates secure listeners.\n\n\n“Datasource not found” Error\nConfig Error. The PostgreSQL driver isn’t loaded, or the datasource XML is malformed.\nVerify that the PostgreSQL JDBC driver is mounted correctly in the container’s shared/resources volume and referenced in server.xml.\n\n\nALB Target Group is “Unhealthy”\nHealth Check Mismatch. The ALB is health-checking the wrong port or protocol.\nVerify the Ingress health check annotations. Ensure alb.ingress.kubernetes.io/healthcheck-protocol is set to HTTPS and points to a valid endpoint (e.g., /decisioncenter/health).",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#deep-dive-tls-ingress",
    "href": "src/solution_overview/troubleshooting.html#deep-dive-tls-ingress",
    "title": "Troubleshooting & Diagnostics",
    "section": "Deep Dive: TLS & Ingress",
    "text": "Deep Dive: TLS & Ingress\nBecause of the strict OPA requirement for HTTPS traffic at the pod level, the connection between the AWS ALB and the Pods is the most fragile configuration point.\n\nInfinite Redirect Loops\nContext: ODM often attempts to redirect traffic internally, which can conflict with ALB redirects. Fix: Ensure the annotation alb.ingress.kubernetes.io/ssl-redirect: '443' is configured. Additionally, verify the ODM Liberty server configuration (server.xml) is configured to trust the proxy headers (X-Forwarded-Proto) coming from the ALB so it recognizes the original request was secure.\n\n\nUntrusted Certificates\nContext: The ALB attempts to connect to the Pod via HTTPS, but the Pod is using a self-signed certificate. Fix: By default, the AWS ALB accepts self-signed certificates from backends when the backend protocol is HTTPS. However, you must ensure the Pod is actually listening on port 9443.\n\n\n\n\n\n\nTipTesting Internal Connectivity\n\n\n\nYou can test the internal TLS handshake from within the cluster using a temporary debug pod.\n# Run a temporary curl pod\nkubectl run curl-debug --image=curlimages/curl -n odm-pilot --rm -it -- sh\n\n# Inside the pod, test the handshake (-k allows self-signed certs)\ncurl -k -v https://&lt;odm-pod-ip&gt;:9443/decisioncenter",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#database-connectivity-checklist",
    "href": "src/solution_overview/troubleshooting.html#database-connectivity-checklist",
    "title": "Troubleshooting & Diagnostics",
    "section": "Database Connectivity Checklist",
    "text": "Database Connectivity Checklist\nIf the Decision Center or Decision Server Console fails to start, 90% of the time it is a Database connectivity or permission issue.\n\n\n\n\n\n\nImportantCritical Checks\n\n\n\n\nVPC Peering/Routing: Ensure the EKS VPC has a valid route table entry to the RDS subnet.\nSecurity Groups:\n\nSource: The Security Group attached to the EKS Worker Nodes.\nDestination: The Security Group attached to RDS (Must allow TCP/5432).\n\nSchema Privileges: If this is a fresh install, ensure the user provided in the JDBC connection string has CREATE TABLE privileges. ODM must create its own schema tables on the very first startup.",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#opa-gatekeeper-diagnostics",
    "href": "src/solution_overview/troubleshooting.html#opa-gatekeeper-diagnostics",
    "title": "Troubleshooting & Diagnostics",
    "section": "OPA Gatekeeper Diagnostics",
    "text": "OPA Gatekeeper Diagnostics\nSince the cluster enforces strict security policies (such as requiring HTTPS listeners), OPA Gatekeeper acts as the admission controller. If your deployments fail validation, use the following commands to investigate why.\n\nIdentifying Policy Violations\nIf a kubectl apply or Helm install fails with an error message like Error from server (Forbidden): admission webhook \"validation.gatekeeper.sh\" denied the request, follow these steps to identify the blocking policy.\n1. List Active Constraints View all constraints currently enforced on the cluster to find the relevant policy (e.g., k8srequiredlabels, k8shttpsonly, etc.).\nkubectl get constraints\n2. Inspect Specific Violations If a resource is blocked or audited, the details are stored in the status field of the Constraint object. This will tell you exactly which field in your manifest failed validation.\n# Syntax: kubectl describe &lt;ConstraintKind&gt; &lt;ConstraintName&gt;\n# Example: Checking for ingress security violations\nkubectl describe k8shttpsonly ingress-must-be-secure\nLook for the Total Violations field and the Violations list in the output.\n\n\nDebugging Policy Logic\nIf you suspect a policy is misconfigured or behaving unexpectedly (e.g., blocking valid resources), you can inspect the underlying Rego logic or check the Gatekeeper controller logs.\nInspect the Constraint Template (Rego) The logic resides in the ConstraintTemplate. Inspecting this allows you to see the actual Rego code being executed.\n# List available templates\nkubectl get constrainttemplates\n\n# View the Rego logic for a specific template\nkubectl get constrainttemplate k8shttpsonly -o yaml\nCheck Gatekeeper Controller Logs The controller logs provide detailed information on webhook admission requests, including the JSON payload that was sent to OPA.\nkubectl logs -l control-plane=controller-manager -n gatekeeper-system\n\n\n\n\n\n\nNoteDry Run Mode\n\n\n\nIf you are debugging a new policy and want to observe violations without blocking deployments, you can temporarily set the enforcement action to dryrun in the Constraint YAML:\nspec:\n  enforcementAction: dryrun\nViolations will appear in the Constraint status (via kubectl describe) but will not block the creation or update of resources.\n\n\n\n\nView All Violations Cluster-Wide\nTo get a comprehensive list of every active policy violation in the cluster (audited vs. blocked), you can use this formatted command. It iterates through every Constraint and prints the resource name and specific error message for each violation.\nkubectl get constraints -o jsonpath='{range .items[*]}{\"POLICY: \"}{.metadata.name}{\"\\n\"}{range .status.violations[*]}{\"  FAIL: \"}{.message}{\"\\n    -&gt; Resource:  \"}{.kind}/{.name}{\"\\n\"}{end}{\"\\n\"}{end}'\nOutput Example:\nPOLICY: ingress-must-be-secure\n  FAIL: Ingress must be https only\n    -&gt; Resource:  Ingress/odm-ingress\n\nPOLICY: container-must-have-probes\n  FAIL: Container &lt;decision-center&gt; is missing a readinessProbe\n    -&gt; Resource:  Pod/odm-decision-center-0",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/implementation_methodology/stepone-imp.html",
    "href": "src/implementation_methodology/stepone-imp.html",
    "title": "Phase 1: Foundation & Prerequisite Configuration",
    "section": "",
    "text": "Before deploying the ODM application logic, the infrastructure foundation must be secured. This phase covers provisioning the database, generating internal TLS assets for end-to-end encryption, and creating the necessary Kubernetes secrets.\n\n\nCreate the dedicated namespace for the ODM deployment. It is recommended to set your current context to this namespace to simplify subsequent commands.\n# 1. Create the namespace\nkubectl create namespace odm-pilot\n\n# 2. Set as current context (Optional but recommended)\nkubectl config set-context --current --namespace=odm-pilot\n\n\n\nODM requires a robust persistence layer. We utilize AWS RDS for PostgreSQL (v12+).\n\nProvision RDS: Ensure the instance is deployed in private subnets reachable by the EKS cluster.\nConfigure Security Groups:\n\nInbound Rule: Allow TCP/5432 from the EKS Cluster Security Group.\nOutbound Rule: Allow return traffic.\n\n\n\n\n\nThe ODM data source requires specific privileges to initialize the schema on the first startup. Connect to your RDS instance via a bastion host or temporary pod and execute the following SQL commands:\n-- 1. Create the dedicated ODM user\nCREATE USER odm WITH PASSWORD 'StrongPassword123!';\n\n-- 2. Create the database\nCREATE DATABASE odm_db OWNER odm;\n\n-- 3. Grant privileges (Required for table creation)\nGRANT ALL PRIVILEGES ON DATABASE odm_db TO odm;\n\n-- 4. (Optional) If using a specific schema\n\\c odm_db\nCREATE SCHEMA odm_rules AUTHORIZATION odm;\n\n\n\nTo satisfy the OPA policy requiring HTTPS traffic at the cluster boundary, the Kubernetes Ingress resource must be configured with a valid TLS secret. This enables the Ingress Controller to terminate HTTPS traffic at the cluster boundary.\nFor this document, we will generate a self-signed certificate using OpenSSL.\nGenerate the Certificate and Key (PEM):\n\n\n\n\n\n\nNoteDetermining the Certificate Subject (CN)\n\n\n\nThe Common Name (/CN) in the certificate must match the exact Fully Qualified Domain Name (FQDN) that users will type into their browser.\n\nIn the Lab: We use the pattern odm.&lt;proxy&gt; (e.g., /CN=odm.my-haproxy.gym.lan/O=Lab/C=US). This ensures the browser accepts the certificate when traffic is routed through your lab’s load balancer.\nIn Production (AWS ALB): Use the Route53 CNAME or Alias record created for the application (e.g., /CN=odm.internal.corp).\n\nImportant: Do not use the raw AWS ALB hostname (e.g., *.elb.amazonaws.com) as the CN. Browser security policies will reject the certificate if it identifies the load balancer hardware rather than the application service name.\n\n\n\n\n# 1. Generate a self-signed certificate and private key\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\\n  -keyout odm-lab.key \\\n  -out odm-lab.crt \\\n  -subj \"/CN=odm.internal.corp/O=MyCorp/C=US\"\n\n# 2. Verify the content\nls -l odm-lab.key odm-lab.crt\n\n\n\nWith the database credentials defined and the keystore generated, inject them into the cluster as Kubernetes Secrets.\nDatabase Credentials Secret:\nThe ODM application requires a Kubernetes Secret to authenticate with the database. Choose the option matching your deployment strategy.\n\nOption A: Production (External RDS)Option B: Lab (Internal DB)\n\n\nTarget: Pilot / Production environment using AWS RDS.\nCreate a secret containing the credentials for your external PostgreSQL instance.\nkubectl create secret generic odm-db-secret \\\n  --namespace odm-pilot \\\n  --from-literal=db-user=odm \\\n  --from-literal=db-password='StrongPassword123!' \\\n  --from-literal=db-name=odm_db \\\n  --from-literal=db-server=postgres.cxxxxx.us-east-1.rds.amazonaws.com\nNote: Ensure the secret name (odm-db-secret) matches the secretCredentials field in your values-prod.yaml.\n\n\nTarget: Sandbox / Local Lab using the internal containerized database.\nCreate a simple secret for the internal PostgreSQL container.\nkubectl create secret generic odm-db-secret \\\n  --namespace odm-pilot \\\n  --from-literal=db-user=odm \\\n  --from-literal=db-password=odm\nNote: Ensure the secret name (odm-db-secret) matches the secretCredentials field in your values-lab.yaml.\n\n\n\nTLS Keystore Secret: This will be referenced by the Ingress resource (tlsSecretRef) to enable HTTPS.\n# Create a standard Kubernetes TLS secret type\nkubectl create secret tls odm-tls-secret \\\n  --namespace odm-pilot \\\n  --key odm-lab.key \\\n  --cert odm-lab.crt\n\n\n\n\n\n\nNoteSecret Management\n\n\n\nIn a production environment, avoid creating secrets from literals in the CLI history. Use an External Secrets Operator (ESO) to sync these values from AWS Secrets Manager or HashiCorp Vault.\n\n\n\n\n\nKubernetes requires authentication credentials to pull container images. Depending on your environment constraints (Lab vs. Restricted Production), the source registry and credentials will differ.\n\nOption A: Private Registry (Production/Restricted)Option B: IBM Registry (Standard Lab)\n\n\nTarget Environment: Customer Pilot / Air-Gapped / OPA-Enforced\nIn strict environments where public internet access is blocked or OPA forbids public registries, you must pull from the internal location where you mirrored the images (e.g., Artifactory).\nAction: Create a secret using your internal registry credentials.\n# Replace with your internal registry details\nkubectl create secret docker-registry internal-registry-secret \\\n  --docker-server=artifactory.internal.corp:8443 \\\n  --docker-username=&lt;SERVICE_ACCOUNT_USER&gt; \\\n  --docker-password=&lt;SERVICE_ACCOUNT_TOKEN&gt; \\\n  --docker-email=admin@internal.corp \\\n  -n odm-pilot\n\n\nTarget Environment: Sandbox / POC with Internet Access\nIf you are working in a lab with direct internet access and no strict OPA registry constraints, you can pull directly from IBM.\nAction: Create a secret using your IBM Entitlement Key.\n# 1. Get your key from myibm.ibm.com/products-services/containerlibrary\n# 2. Create the secret\nkubectl create secret docker-registry internal-registry-secret \\\n  --docker-server=cp.icr.io \\\n  --docker-username=cp \\\n  --docker-password=&lt;YOUR_IBM_ENTITLEMENT_KEY&gt; \\\n  --docker-email=user@example.com \\\n  -n odm-pilot\n\n\n\n\n\n\n\n\n\nImportantSecret Name Consistency\n\n\n\nWhichever option you choose, ensure the secret name used in the kubectl create command exactly matches the value in your values.yaml file under image.pullSecrets.",
    "crumbs": [
      "Implementation Methodology",
      "Foundation & Prereqs"
    ]
  },
  {
    "objectID": "src/implementation_methodology/stepone-imp.html#phase-1-foundation-prerequisite-configuration",
    "href": "src/implementation_methodology/stepone-imp.html#phase-1-foundation-prerequisite-configuration",
    "title": "Phase 1: Foundation & Prerequisite Configuration",
    "section": "",
    "text": "Before deploying the ODM application logic, the infrastructure foundation must be secured. This phase covers provisioning the database, generating internal TLS assets for end-to-end encryption, and creating the necessary Kubernetes secrets.\n\n\nCreate the dedicated namespace for the ODM deployment. It is recommended to set your current context to this namespace to simplify subsequent commands.\n# 1. Create the namespace\nkubectl create namespace odm-pilot\n\n# 2. Set as current context (Optional but recommended)\nkubectl config set-context --current --namespace=odm-pilot\n\n\n\nODM requires a robust persistence layer. We utilize AWS RDS for PostgreSQL (v12+).\n\nProvision RDS: Ensure the instance is deployed in private subnets reachable by the EKS cluster.\nConfigure Security Groups:\n\nInbound Rule: Allow TCP/5432 from the EKS Cluster Security Group.\nOutbound Rule: Allow return traffic.\n\n\n\n\n\nThe ODM data source requires specific privileges to initialize the schema on the first startup. Connect to your RDS instance via a bastion host or temporary pod and execute the following SQL commands:\n-- 1. Create the dedicated ODM user\nCREATE USER odm WITH PASSWORD 'StrongPassword123!';\n\n-- 2. Create the database\nCREATE DATABASE odm_db OWNER odm;\n\n-- 3. Grant privileges (Required for table creation)\nGRANT ALL PRIVILEGES ON DATABASE odm_db TO odm;\n\n-- 4. (Optional) If using a specific schema\n\\c odm_db\nCREATE SCHEMA odm_rules AUTHORIZATION odm;\n\n\n\nTo satisfy the OPA policy requiring HTTPS traffic at the cluster boundary, the Kubernetes Ingress resource must be configured with a valid TLS secret. This enables the Ingress Controller to terminate HTTPS traffic at the cluster boundary.\nFor this document, we will generate a self-signed certificate using OpenSSL.\nGenerate the Certificate and Key (PEM):\n\n\n\n\n\n\nNoteDetermining the Certificate Subject (CN)\n\n\n\nThe Common Name (/CN) in the certificate must match the exact Fully Qualified Domain Name (FQDN) that users will type into their browser.\n\nIn the Lab: We use the pattern odm.&lt;proxy&gt; (e.g., /CN=odm.my-haproxy.gym.lan/O=Lab/C=US). This ensures the browser accepts the certificate when traffic is routed through your lab’s load balancer.\nIn Production (AWS ALB): Use the Route53 CNAME or Alias record created for the application (e.g., /CN=odm.internal.corp).\n\nImportant: Do not use the raw AWS ALB hostname (e.g., *.elb.amazonaws.com) as the CN. Browser security policies will reject the certificate if it identifies the load balancer hardware rather than the application service name.\n\n\n\n\n# 1. Generate a self-signed certificate and private key\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\\n  -keyout odm-lab.key \\\n  -out odm-lab.crt \\\n  -subj \"/CN=odm.internal.corp/O=MyCorp/C=US\"\n\n# 2. Verify the content\nls -l odm-lab.key odm-lab.crt\n\n\n\nWith the database credentials defined and the keystore generated, inject them into the cluster as Kubernetes Secrets.\nDatabase Credentials Secret:\nThe ODM application requires a Kubernetes Secret to authenticate with the database. Choose the option matching your deployment strategy.\n\nOption A: Production (External RDS)Option B: Lab (Internal DB)\n\n\nTarget: Pilot / Production environment using AWS RDS.\nCreate a secret containing the credentials for your external PostgreSQL instance.\nkubectl create secret generic odm-db-secret \\\n  --namespace odm-pilot \\\n  --from-literal=db-user=odm \\\n  --from-literal=db-password='StrongPassword123!' \\\n  --from-literal=db-name=odm_db \\\n  --from-literal=db-server=postgres.cxxxxx.us-east-1.rds.amazonaws.com\nNote: Ensure the secret name (odm-db-secret) matches the secretCredentials field in your values-prod.yaml.\n\n\nTarget: Sandbox / Local Lab using the internal containerized database.\nCreate a simple secret for the internal PostgreSQL container.\nkubectl create secret generic odm-db-secret \\\n  --namespace odm-pilot \\\n  --from-literal=db-user=odm \\\n  --from-literal=db-password=odm\nNote: Ensure the secret name (odm-db-secret) matches the secretCredentials field in your values-lab.yaml.\n\n\n\nTLS Keystore Secret: This will be referenced by the Ingress resource (tlsSecretRef) to enable HTTPS.\n# Create a standard Kubernetes TLS secret type\nkubectl create secret tls odm-tls-secret \\\n  --namespace odm-pilot \\\n  --key odm-lab.key \\\n  --cert odm-lab.crt\n\n\n\n\n\n\nNoteSecret Management\n\n\n\nIn a production environment, avoid creating secrets from literals in the CLI history. Use an External Secrets Operator (ESO) to sync these values from AWS Secrets Manager or HashiCorp Vault.\n\n\n\n\n\nKubernetes requires authentication credentials to pull container images. Depending on your environment constraints (Lab vs. Restricted Production), the source registry and credentials will differ.\n\nOption A: Private Registry (Production/Restricted)Option B: IBM Registry (Standard Lab)\n\n\nTarget Environment: Customer Pilot / Air-Gapped / OPA-Enforced\nIn strict environments where public internet access is blocked or OPA forbids public registries, you must pull from the internal location where you mirrored the images (e.g., Artifactory).\nAction: Create a secret using your internal registry credentials.\n# Replace with your internal registry details\nkubectl create secret docker-registry internal-registry-secret \\\n  --docker-server=artifactory.internal.corp:8443 \\\n  --docker-username=&lt;SERVICE_ACCOUNT_USER&gt; \\\n  --docker-password=&lt;SERVICE_ACCOUNT_TOKEN&gt; \\\n  --docker-email=admin@internal.corp \\\n  -n odm-pilot\n\n\nTarget Environment: Sandbox / POC with Internet Access\nIf you are working in a lab with direct internet access and no strict OPA registry constraints, you can pull directly from IBM.\nAction: Create a secret using your IBM Entitlement Key.\n# 1. Get your key from myibm.ibm.com/products-services/containerlibrary\n# 2. Create the secret\nkubectl create secret docker-registry internal-registry-secret \\\n  --docker-server=cp.icr.io \\\n  --docker-username=cp \\\n  --docker-password=&lt;YOUR_IBM_ENTITLEMENT_KEY&gt; \\\n  --docker-email=user@example.com \\\n  -n odm-pilot\n\n\n\n\n\n\n\n\n\nImportantSecret Name Consistency\n\n\n\nWhichever option you choose, ensure the secret name used in the kubectl create command exactly matches the value in your values.yaml file under image.pullSecrets.",
    "crumbs": [
      "Implementation Methodology",
      "Foundation & Prereqs"
    ]
  },
  {
    "objectID": "src/implementation_methodology/stepthree-imp.html",
    "href": "src/implementation_methodology/stepthree-imp.html",
    "title": "Step Three",
    "section": "",
    "text": "Step Three Implementation\nPhasellus at risus egestas, ultricies tortor efficitur, auctor augue. Suspendisse finibus maximus dui nec condimentum. Proin fringilla efficitur vehicula. Suspendisse sem lacus, iaculis quis erat et, facilisis sagittis est. Sed sapien justo, condimentum vitae aliquet sed, faucibus at ex. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Fusce placerat ante eget diam tincidunt, vitae tincidunt sapien malesuada. Nullam ullamcorper justo eros. Duis ultricies aliquam dui, id aliquam lorem congue vitae. Ut porttitor, tellus eu dignissim semper, turpis nunc cursus libero, sed placerat est dui non enim.\nVestibulum libero dolor, vehicula vitae risus non, consequat gravida neque. Maecenas a posuere sem. Quisque dignissim porta pretium. Nam mattis lacus commodo lobortis consequat. Vestibulum at massa a quam egestas accumsan. Fusce ac neque eu libero maximus aliquet id quis metus. In sodales neque ut turpis iaculis vehicula. Sed volutpat, tellus non laoreet aliquet, risus felis ornare dolor, interdum venenatis turpis metus in lorem. Aliquam quam ligula, porttitor non ultrices ac, lacinia ullamcorper massa. Integer molestie eleifend urna, imperdiet dignissim tellus malesuada ac. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tristique sem ac purus interdum, vitae pharetra erat fringilla. Integer non nunc a eros porttitor rutrum. Suspendisse ut libero urna. Vestibulum vitae est at diam vestibulum aliquet. Donec porta nunc eget lobortis mollis.\nVestibulum luctus sodales odio, at luctus lacus lobortis a. Aliquam vulputate id turpis sit amet bibendum. Donec a dignissim tellus, vel vehicula erat. Pellentesque hendrerit ex magna, at dictum est pretium a. Vivamus faucibus ipsum lectus, ut elementum diam interdum scelerisque. Cras magna sapien, tincidunt quis eleifend at, tincidunt non eros. Aliquam ac augue turpis."
  }
]