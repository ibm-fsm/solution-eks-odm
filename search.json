[
  {
    "objectID": "src/implementation_methodology/steptwo-imp.html",
    "href": "src/implementation_methodology/steptwo-imp.html",
    "title": "ODM Deployment & Traffic Exposure",
    "section": "",
    "text": "In this final phase, we deploy the ODM workload. The configuration must bridge the gap between the Kubernetes Service (ClusterIP) and the AWS ALB, ensuring traffic remains encrypted across the boundary.\n\n\nBefore generating the deployment manifests, add the IBM Helm repository to your local client. This allows Helm to locate the ibm-odm-prod chart.\n# 1. Add the IBM Helm Repo\nhelm repo add ibm-helm https://raw.githubusercontent.com/IBM/charts/master/repo/ibm-helm\n\n# 2. Update to ensure you have the latest chart versions\nhelm repo update\n\n# 3. Verify the chart is available (Target: 25.1.0 for ODM 9.5.0.1)\nhelm search repo ibm-odm-prod\n\n\n\n\n\n\nTipReference Documentation\n\n\n\nFor a complete list of available configuration parameters, default values, and architectural details, refer to the official IBM ODM Production Helm Chart README.\n\n\n\n\n\nTo satisfy the strict OPA policies and network requirements, we must construct a specific values.yaml file. This file overrides the default “insecure” settings of the chart.\n\n\nSelect the configuration that matches your deployment phase.\n\nOption A: Production / Pilot (Target State)Option B: Lab Validation (Internal DB)\n\n\nUse Case: Deployment into the EKS Pilot environment.\nKey Features: External Oracle Database, Image Digests, Strict Security Contexts.\nCreate a file named values-prod.yaml:\nDownload values-prod.yaml\n# values-prod.yaml\n\n# 1. License & Auth\nlicense: true\nusersPassword: \"&lt;SET_ADMIN_PASSWORD&gt;\"\n\n# 2. Image Config (Internal Mirror)\nimage:\n  repository: artifactory.internal.corp/odm-repo\n  pullSecrets:\n    - internal-registry-secret\n  # Note: Global tag is commented out to force component-level digests\n  # tag: \"9.5.0.1\"\n\n# 3. Component Digests (Required for 'container-image-must-have-digest' policy)\n# You must obtain the SHA256 digest from your Artifactory for each image.\ndecisionCenter:\n  tagOrDigest: \"sha256:6a0eb1f874ba52918bcd8e2c3acde2d3e428685cad7e5996e0c1227e88d3de0b\"\ndecisionRunner:\n  tagOrDigest: \"sha256:6f0643013e18d848199a73f38c5f6f854c1226ae7702c8294b835b74aa561782\"\ndecisionServerConsole:\n  tagOrDigest: \"sha256:f4c778a388535330ce5d5612d6325d5522cedb70f0cb7895fa7f015a38e5bb9c\"\ndecisionServerRuntime:\n  tagOrDigest: \"sha256:ab03e4e35923c674a090456f6869963a6d29e8f94117061ff11d383cc8c9369a\"\n\n# 4. Architecture: External Database (Required for 'psp-fsgroup' policy)\ninternalDatabase:\n  persistence:\n    enabled: false # Disable internal DB\n\nexternalDatabase:\n  type: \"oracle\" # or \"postgresql\"\n  url: \"jdbc:oracle:thin:@//&lt;ORACLE_SERVER_ADDRESS&gt;:1521/freepdb1\"\"\n  # References the secret created in Prereqs section\n  secretCredentials: \"odm-db-secret\"\n\n# 5. Security Contexts (Native v9.5 Features)\ncustomization:\n  runAsUser: 1001\n  # NATIVE FIX: Satisfies psp-seccomp\n  seccompProfile:\n    type: RuntimeDefault\n  # NATIVE FIX: Satisfies must-have-appid\n  labels:\n    applicationid: \"ODM-PILOT\"\n\n# 6. Ingress Configuration (AWS ALB Specific)\nservice:\n  type: ClusterIP\n  enableRoute: false\n  # NATIVE FIX: Satisfies \"Host cannot be empty\" policy\n  hostname: \"odm.internal.corp\"\n\n  ingress:\n    enabled: true\n    host: \"odm.internal.corp\"\n    \n    # NOTE: When using AWS ACM, we do not use k8s TLS secrets. \n    # However, if OPA strictness requires a TLS block to be present in the YAML, \n    # leave these empty or define a dummy secret. \n    # Usually, the 'allow-http: false' annotation satisfies OPA.\n    tlsSecretRef: \"\" \n    tlsHosts: []\n\n    annotations:\n      # 1. Controller Class\n      kubernetes.io/ingress.class: alb\n      \n      # 2. Network Configuration\n      alb.ingress.kubernetes.io/scheme: internet-facing\n      # 'ip' mode routes traffic directly to Pod IPs (bypassing NodePort)\n      # This is faster and required for some sticky session configurations\n      alb.ingress.kubernetes.io/target-type: ip\n      \n      # 3. Encryption & Certificates (AWS ACM)\n      # Reference your ACM Certificate ARN here\n      alb.ingress.kubernetes.io/certificate-arn: \"arn:aws:acm:us-east-1:123456789012:certificate/xxxx-xxxx-xxxx\"\n      # Listen on HTTPS (443)\n      alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n      \n      # 4. Backend Security (Re-encryption)\n      # Tells ALB to speak HTTPS to the Pods (required for OPA compliance inside cluster)\n      alb.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n      # Ensure Health Checks also use HTTPS so pods don't fail readiness\n      alb.ingress.kubernetes.io/healthcheck-protocol: \"HTTPS\"\n      \n      # 5. OPA Compliance\n      # Explicitly disables HTTP to satisfy 'ingress-https-only' policy\n      kubernetes.io/ingress.allow-http: \"false\"\n\n\n\n\n\n\nImportantAWS ALB & OPA “TLS” Constraints\n\n\n\nStandard OPA policies (ingress-https-only) often check if the spec.tls list is populated in the Ingress YAML.\nIn AWS ALB: You typically use the certificate-arn annotation instead of a Kubernetes Secret, leaving spec.tls empty.\nIf OPA blocks this configuration: You may need to create a “dummy” self-signed secret and reference it in tlsSecretRef just to satisfy the OPA regex check, even though the ALB ignores it in favor of the ARN.\n\n\n\n\nUse Case: Sandbox testing where an external database is not available.\nKey Features: Internal PostgreSQL or Oracle (Accepts known OPA violation for DB only), Image Digests, Self-Signed Ingress.\nCreate a file named values-lab.yaml:\nDownload values-lab.yaml\n# values-lab.yaml\n\n# 1. License & Auth\nlicense: true\nusersPassword: \"odmAdminPassword123!\"\n\n# 2. Image Config (Lab Artifactory)\nimage:\n  # document out or remove the repository line to use the default IBM registry (cp.icr.io)\n  repository: artifactory.gym.lan:8443/docker-local\n  pullSecrets:\n    - internal-registry-secret\n  # Do not use tag for lab deployments - use digest instead\n  # tag: \"9.5.0.1\"\n\n# 3. Component Digests (From Lab Artifactory)\ndecisionCenter:\n  tagOrDigest: \"sha256:6a0eb1f874ba52918bcd8e2c3acde2d3e428685cad7e5996e0c1227e88d3de0b\"\ndecisionRunner:\n  tagOrDigest: \"sha256:6f0643013e18d848199a73f38c5f6f854c1226ae7702c8294b835b74aa561782\"\ndecisionServerConsole:\n  tagOrDigest: \"sha256:f4c778a388535330ce5d5612d6325d5522cedb70f0cb7895fa7f015a38e5bb9c\"\ndecisionServerRuntime:\n  tagOrDigest: \"sha256:ab03e4e35923c674a090456f6869963a6d29e8f94117061ff11d383cc8c9369a\"\n\n# 4. Architecture: Internal Database\n# Note: This WILL fail 'psp-fsgroup' checks. Acceptable for Lab only.\ninternalDatabase:\n  # Digest for dbserver image\n  tagOrDigest: \"sha256:9106481ba539808ea9fed4b7d3197e91732748bc2170e862b729af8cc874f5db\"\n  persistence:\n    enabled: true\n    useDynamicProvisioning: true\n    storageClassName: \"local-path\"\n  runAsUser: 26\n# Uncomment below to use \"external\" DB in lab setting, ensure that internalDatabase above is commented out\n# internalDatabase:\n#   persistence:\n#     enabled: false # Disable internal DB\n# externalDatabase:\n#   type: \"postgresql\"\n#   serverName: \"postgres.postgres.svc.cluster.local\"\n#   databaseName: \"odm_db\"\n#   port: \"5432\"\n#   # References the secret created in Prereqs section\n#   secretCredentials: \"odm-db-secret\"\n# externalDatabase:\n#   type: \"oracle\"\n#   url: \"jdbc:oracle:thin:@//oracle-db.oracle.svc.cluster.local:1521/freepdb1\"\n#   # References the secret created in Prereqs section\n#   secretCredentials: \"odm-db-secret\"\n\n# 5. Security Contexts\ncustomization:\n  runAsUser: 1001\n  seccompProfile:\n    type: RuntimeDefault\n  labels:\n    applicationid: \"ODM-LAB\"\n\n# 6. Ingress Configuration for lab using nginx ingress controller\nservice:\n  type: ClusterIP\n  enableRoute: false\n  hostname: \"odm.my-haproxy.gym.lan\"\n\n  ingress:\n    enabled: true\n    host: \"odm.my-haproxy.gym.lan\"\n    tlsSecretRef: \"odm-tls-secret\"\n    tlsHosts:\n      - \"odm.my-haproxy.gym.lan\"\n    annotations:\n      kubernetes.io/ingress.class: nginx\n      kubernetes.io/ingress.allow-http: \"false\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n\n\n\n\n\n\n\nWhile ODM v9.5 resolves many security configurations natively, critical gaps remain that cannot be fixed via values.yaml alone:\n1. Group ID Enforcement: The Helm chart templates explicitly define runAsUser but ignore runAsGroup and supplementalGroups. This causes the pods to fail the psp-pods-allowed-user-ranges policy. We inject these fields (1001) into every Deployment. 2. Test Job Compliance: The odm-test-connection Job generated by the chart is unconfigurable via values.yaml. It lacks Resource Limits, Security Contexts, and Image Digests. We patch this job to add all missing security fields. 3. Image Pull Policy (Init Containers): The OPA policy requires imagePullPolicy: Always for all containers. The Helm chart defaults Init Containers to IfNotPresent with no option to override. We use a JSON Patch to forcibly update the pull policy on every container in the manifest.\n\n\nDefine the following files in your overlay directory. Note that we utilize distinct patch files for each component to ensure explicit targeting and avoid accidental modification of infrastructure components (such as the database).\n\n\nTarget: Application Deployments (Decision Center, Runner, Console, Runtime).\nPurpose: Injects the mandatory Group IDs required by the “Restricted” OPA policy.\nDownload security-patch.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: .*\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsUser: 1001\n        runAsGroup: 1001\n        supplementalGroups: [1001]\n\n\n\nTarget: The Database Connection Test Job.\nPurpose: This job is “unconfigurable” in the standard chart. We must patch it to enforce Image Digests, Resource Limits, and strict Security Contexts.\nDownload job-security-patch.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: .*\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-lab-odm-test: runtime/default  \n    spec:\n      # Pod Level Security\n      securityContext:\n        runAsUser: 1001\n        runAsGroup: 1001\n        supplementalGroups: [1001]\n        seccompProfile:\n          type: RuntimeDefault\n\n      containers:\n      - name: odm-lab-odm-test\n        # CRITICAL: The Helm chart does not support digests for this specific job.\n        # You must hardcode the mirrored Runtime image and SHA256 digest here or comment out to use default cp.icr.io\n        image: artifactory.internal.corp/odm-repo/odm-decisionserverruntime@sha256:ab03e4e35923c674a090456f6869963a6d29e8f94117061ff11d383cc8c9369a\n        # Use the following for default image from IBM Container Registry\n        #image: cp.icr.io/cp/cp4a/odm/odm-decisionserverruntime@sha256:ab03e4e35923c674a090456f6869963a6d29e8f94117061ff11d383cc8c9369a\n\n        # Fix \"container-must-have-limits-and-requests\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n\n        # Fix \"privilege-escalation\", \"capabilities\", \"readonlyrootfilesystem\"\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n\n\n\nTarget: Decision Center Deployment.\nPurpose: Injects the mandatory runAsGroup: 1001 and supplementalGroups into the Decision Center pods, as the Helm chart template ignores these values for this component.\nDownload security-patch-dc.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisioncenter\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisioncenter: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisioncenter: runtime/default\n\n\n\nTarget: Decision Runner Deployment.\nPurpose: Injects the mandatory runAsGroup: 1001 and supplementalGroups into the Decision Runner pods to satisfy the “Restricted” OPA policy.\nDownload security-patch-runner.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionrunner\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionrunner: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionrunner: runtime/default\n\n\n\nTarget: Decision Server Console Deployment.\nPurpose: Injects the mandatory runAsGroup: 1001 and supplementalGroups into the Rule Execution Server Console pods.\nDownload security-patch-console.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionserverconsole\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionserverconsole: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionserverconsole: runtime/default\n\n\n\nTarget: Decision Server Runtime Deployment.\nPurpose: Injects the mandatory runAsGroup: 1001 and supplementalGroups into the Rule Execution Server Runtime pods.\nDownload security-patch-runtime.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionserverruntime\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionserverruntime: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionserverruntime: runtime/default\n\n\n\nTarget: All ODM Deployments and Jobs.\nPurpose: A JSON Patch that overrides the imagePullPolicy for all containers (including Init Containers, which cannot be configured via Helm). This forces the policy to Always, satisfying strict image currency requirements.\nDownload force-pull-policy.yaml\n# force-pull-policy.yaml\n# Forces Main Container\n- op: replace\n  path: /spec/template/spec/containers/0/imagePullPolicy\n  value: Always\n# Forces First Init Container (e.g. init-folder-readonlyfs)\n- op: replace\n  path: /spec/template/spec/initContainers/0/imagePullPolicy\n  value: Always\n# Forces Second Init Container (e.g. init-decisionrunner)\n# Note: This needs uncommented if using internal database in a lab setting for OPA constraint\n# - op: replace\n#   path: /spec/template/spec/initContainers/1/imagePullPolicy\n#   value: Always\n\n\n\n\nCreate the kustomization.yaml file to link the patches to the generated resources.\nImportant: We use explicit name targeting to ensure we do not accidentally patch the Database Deployment (if running locally) or other infrastructure components.\nNote: The names below assume your Helm Release Name is odm-lab. If you use odm-pilot, update the names accordingly (e.g., odm-pilot-odm-decisioncenter).\nDownload kustomization.yaml\nresources:\n  - odm-raw.yaml\n\npatches:\n  # Generic Security Patch (Group IDs + Token)\n  # Targets ALL deployments via regex name\n  - path: security-patch.yaml\n    target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: \"odm-lab-odm-.*\" # \"odm-pilot-odm-.*\"\n\n  # Specific AppArmor Patches (One per file)\n  - path: security-patch-dc.yaml\n  - path: security-patch-runner.yaml\n  - path: security-patch-console.yaml\n  - path: security-patch-runtime.yaml\n\n  # Test Job Patch\n  - path: job-security-patch.yaml\n    target:\n      group: batch\n      version: v1\n      kind: Job\n      name: odm-lab-odm-test # \"odm-pilot-odm-test\"\n\n  # Force Pull Policy (JSON Patch)\n  # Applies to all ODM deployments\n  - path: force-pull-policy.yaml\n    target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: \"odm-lab-odm-.*\" # \"odm-pilot-odm-.*\"\n\n\n\n\n\n\nTipLab Workaround: Non-AppArmor Hosts (RHEL/CentOS/Rocky)\n\n\n\nIf you are running this deployment in a lab environment based on RHEL, CentOS, or Rocky Linux, your kernel likely uses SELinux instead of AppArmor.\nIncluding the container.apparmor.security.beta.kubernetes.io annotations (which are mandatory for the restricted EKS environment) will cause your lab pods to hang in a Blocked state with the error: Cannot enforce AppArmor: AppArmor is not enabled on the host.\nAction: Run the following command to strip these annotations from all YAML files in your current directory before applying the configuration:\nsed -i '/container.apparmor.security.beta.kubernetes.io/d' *.yaml\n\n\n\n\n\n\nRun the build pipeline to generate the patched manifests and apply them to the cluster.\n# 1. Render Helm Template\n# (Change values-prod.yaml to values-lab.yaml if needed)\nhelm template odm-lab ibm-helm/ibm-odm-prod \\\n  --version 25.1.0 \\\n  --kube-version 1.28.0 \\\n  -f values-prod.yaml &gt; odm-raw.yaml\n\n# 2. Apply Patches & Deploy\nkubectl -n odm-pilot apply -k .\n\n\n\n\n\n\nTipVerification\n\n\n\nAfter deployment, verify that the ALB has successfully registered the targets.\nkubectl get ingress -n odm-pilot\n# Look for the ADDRESS field (e.g., k8s-odmpilot-xxxx.us-east-1.elb.amazonaws.com)\nNavigate to https://odm.mycompany.com/decisioncenter. If the page loads securely, End-to-End encryption is functioning correctly.\nUse odmAdmin and the password you set in the values.yaml to log in (e.g. odmAdminPassword123!).",
    "crumbs": [
      "Implementation Methodology",
      "ODM Deployment"
    ]
  },
  {
    "objectID": "src/implementation_methodology/steptwo-imp.html#phase-2-odm-deployment-traffic-exposure",
    "href": "src/implementation_methodology/steptwo-imp.html#phase-2-odm-deployment-traffic-exposure",
    "title": "ODM Deployment & Traffic Exposure",
    "section": "",
    "text": "In this final phase, we deploy the ODM workload. The configuration must bridge the gap between the Kubernetes Service (ClusterIP) and the AWS ALB, ensuring traffic remains encrypted across the boundary.\n\n\nBefore generating the deployment manifests, add the IBM Helm repository to your local client. This allows Helm to locate the ibm-odm-prod chart.\n# 1. Add the IBM Helm Repo\nhelm repo add ibm-helm https://raw.githubusercontent.com/IBM/charts/master/repo/ibm-helm\n\n# 2. Update to ensure you have the latest chart versions\nhelm repo update\n\n# 3. Verify the chart is available (Target: 25.1.0 for ODM 9.5.0.1)\nhelm search repo ibm-odm-prod\n\n\n\n\n\n\nTipReference Documentation\n\n\n\nFor a complete list of available configuration parameters, default values, and architectural details, refer to the official IBM ODM Production Helm Chart README.\n\n\n\n\n\nTo satisfy the strict OPA policies and network requirements, we must construct a specific values.yaml file. This file overrides the default “insecure” settings of the chart.\n\n\nSelect the configuration that matches your deployment phase.\n\nOption A: Production / Pilot (Target State)Option B: Lab Validation (Internal DB)\n\n\nUse Case: Deployment into the EKS Pilot environment.\nKey Features: External Oracle Database, Image Digests, Strict Security Contexts.\nCreate a file named values-prod.yaml:\nDownload values-prod.yaml\n# values-prod.yaml\n\n# 1. License & Auth\nlicense: true\nusersPassword: \"&lt;SET_ADMIN_PASSWORD&gt;\"\n\n# 2. Image Config (Internal Mirror)\nimage:\n  repository: artifactory.internal.corp/odm-repo\n  pullSecrets:\n    - internal-registry-secret\n  # Note: Global tag is commented out to force component-level digests\n  # tag: \"9.5.0.1\"\n\n# 3. Component Digests (Required for 'container-image-must-have-digest' policy)\n# You must obtain the SHA256 digest from your Artifactory for each image.\ndecisionCenter:\n  tagOrDigest: \"sha256:6a0eb1f874ba52918bcd8e2c3acde2d3e428685cad7e5996e0c1227e88d3de0b\"\ndecisionRunner:\n  tagOrDigest: \"sha256:6f0643013e18d848199a73f38c5f6f854c1226ae7702c8294b835b74aa561782\"\ndecisionServerConsole:\n  tagOrDigest: \"sha256:f4c778a388535330ce5d5612d6325d5522cedb70f0cb7895fa7f015a38e5bb9c\"\ndecisionServerRuntime:\n  tagOrDigest: \"sha256:ab03e4e35923c674a090456f6869963a6d29e8f94117061ff11d383cc8c9369a\"\n\n# 4. Architecture: External Database (Required for 'psp-fsgroup' policy)\ninternalDatabase:\n  persistence:\n    enabled: false # Disable internal DB\n\nexternalDatabase:\n  type: \"oracle\" # or \"postgresql\"\n  url: \"jdbc:oracle:thin:@//&lt;ORACLE_SERVER_ADDRESS&gt;:1521/freepdb1\"\"\n  # References the secret created in Prereqs section\n  secretCredentials: \"odm-db-secret\"\n\n# 5. Security Contexts (Native v9.5 Features)\ncustomization:\n  runAsUser: 1001\n  # NATIVE FIX: Satisfies psp-seccomp\n  seccompProfile:\n    type: RuntimeDefault\n  # NATIVE FIX: Satisfies must-have-appid\n  labels:\n    applicationid: \"ODM-PILOT\"\n\n# 6. Ingress Configuration (AWS ALB Specific)\nservice:\n  type: ClusterIP\n  enableRoute: false\n  # NATIVE FIX: Satisfies \"Host cannot be empty\" policy\n  hostname: \"odm.internal.corp\"\n\n  ingress:\n    enabled: true\n    host: \"odm.internal.corp\"\n    \n    # NOTE: When using AWS ACM, we do not use k8s TLS secrets. \n    # However, if OPA strictness requires a TLS block to be present in the YAML, \n    # leave these empty or define a dummy secret. \n    # Usually, the 'allow-http: false' annotation satisfies OPA.\n    tlsSecretRef: \"\" \n    tlsHosts: []\n\n    annotations:\n      # 1. Controller Class\n      kubernetes.io/ingress.class: alb\n      \n      # 2. Network Configuration\n      alb.ingress.kubernetes.io/scheme: internet-facing\n      # 'ip' mode routes traffic directly to Pod IPs (bypassing NodePort)\n      # This is faster and required for some sticky session configurations\n      alb.ingress.kubernetes.io/target-type: ip\n      \n      # 3. Encryption & Certificates (AWS ACM)\n      # Reference your ACM Certificate ARN here\n      alb.ingress.kubernetes.io/certificate-arn: \"arn:aws:acm:us-east-1:123456789012:certificate/xxxx-xxxx-xxxx\"\n      # Listen on HTTPS (443)\n      alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n      \n      # 4. Backend Security (Re-encryption)\n      # Tells ALB to speak HTTPS to the Pods (required for OPA compliance inside cluster)\n      alb.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n      # Ensure Health Checks also use HTTPS so pods don't fail readiness\n      alb.ingress.kubernetes.io/healthcheck-protocol: \"HTTPS\"\n      \n      # 5. OPA Compliance\n      # Explicitly disables HTTP to satisfy 'ingress-https-only' policy\n      kubernetes.io/ingress.allow-http: \"false\"\n\n\n\n\n\n\nImportantAWS ALB & OPA “TLS” Constraints\n\n\n\nStandard OPA policies (ingress-https-only) often check if the spec.tls list is populated in the Ingress YAML.\nIn AWS ALB: You typically use the certificate-arn annotation instead of a Kubernetes Secret, leaving spec.tls empty.\nIf OPA blocks this configuration: You may need to create a “dummy” self-signed secret and reference it in tlsSecretRef just to satisfy the OPA regex check, even though the ALB ignores it in favor of the ARN.\n\n\n\n\nUse Case: Sandbox testing where an external database is not available.\nKey Features: Internal PostgreSQL or Oracle (Accepts known OPA violation for DB only), Image Digests, Self-Signed Ingress.\nCreate a file named values-lab.yaml:\nDownload values-lab.yaml\n# values-lab.yaml\n\n# 1. License & Auth\nlicense: true\nusersPassword: \"odmAdminPassword123!\"\n\n# 2. Image Config (Lab Artifactory)\nimage:\n  # document out or remove the repository line to use the default IBM registry (cp.icr.io)\n  repository: artifactory.gym.lan:8443/docker-local\n  pullSecrets:\n    - internal-registry-secret\n  # Do not use tag for lab deployments - use digest instead\n  # tag: \"9.5.0.1\"\n\n# 3. Component Digests (From Lab Artifactory)\ndecisionCenter:\n  tagOrDigest: \"sha256:6a0eb1f874ba52918bcd8e2c3acde2d3e428685cad7e5996e0c1227e88d3de0b\"\ndecisionRunner:\n  tagOrDigest: \"sha256:6f0643013e18d848199a73f38c5f6f854c1226ae7702c8294b835b74aa561782\"\ndecisionServerConsole:\n  tagOrDigest: \"sha256:f4c778a388535330ce5d5612d6325d5522cedb70f0cb7895fa7f015a38e5bb9c\"\ndecisionServerRuntime:\n  tagOrDigest: \"sha256:ab03e4e35923c674a090456f6869963a6d29e8f94117061ff11d383cc8c9369a\"\n\n# 4. Architecture: Internal Database\n# Note: This WILL fail 'psp-fsgroup' checks. Acceptable for Lab only.\ninternalDatabase:\n  # Digest for dbserver image\n  tagOrDigest: \"sha256:9106481ba539808ea9fed4b7d3197e91732748bc2170e862b729af8cc874f5db\"\n  persistence:\n    enabled: true\n    useDynamicProvisioning: true\n    storageClassName: \"local-path\"\n  runAsUser: 26\n# Uncomment below to use \"external\" DB in lab setting, ensure that internalDatabase above is commented out\n# internalDatabase:\n#   persistence:\n#     enabled: false # Disable internal DB\n# externalDatabase:\n#   type: \"postgresql\"\n#   serverName: \"postgres.postgres.svc.cluster.local\"\n#   databaseName: \"odm_db\"\n#   port: \"5432\"\n#   # References the secret created in Prereqs section\n#   secretCredentials: \"odm-db-secret\"\n# externalDatabase:\n#   type: \"oracle\"\n#   url: \"jdbc:oracle:thin:@//oracle-db.oracle.svc.cluster.local:1521/freepdb1\"\n#   # References the secret created in Prereqs section\n#   secretCredentials: \"odm-db-secret\"\n\n# 5. Security Contexts\ncustomization:\n  runAsUser: 1001\n  seccompProfile:\n    type: RuntimeDefault\n  labels:\n    applicationid: \"ODM-LAB\"\n\n# 6. Ingress Configuration for lab using nginx ingress controller\nservice:\n  type: ClusterIP\n  enableRoute: false\n  hostname: \"odm.my-haproxy.gym.lan\"\n\n  ingress:\n    enabled: true\n    host: \"odm.my-haproxy.gym.lan\"\n    tlsSecretRef: \"odm-tls-secret\"\n    tlsHosts:\n      - \"odm.my-haproxy.gym.lan\"\n    annotations:\n      kubernetes.io/ingress.class: nginx\n      kubernetes.io/ingress.allow-http: \"false\"\n      nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n      nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\"\n\n\n\n\n\n\n\nWhile ODM v9.5 resolves many security configurations natively, critical gaps remain that cannot be fixed via values.yaml alone:\n1. Group ID Enforcement: The Helm chart templates explicitly define runAsUser but ignore runAsGroup and supplementalGroups. This causes the pods to fail the psp-pods-allowed-user-ranges policy. We inject these fields (1001) into every Deployment. 2. Test Job Compliance: The odm-test-connection Job generated by the chart is unconfigurable via values.yaml. It lacks Resource Limits, Security Contexts, and Image Digests. We patch this job to add all missing security fields. 3. Image Pull Policy (Init Containers): The OPA policy requires imagePullPolicy: Always for all containers. The Helm chart defaults Init Containers to IfNotPresent with no option to override. We use a JSON Patch to forcibly update the pull policy on every container in the manifest.\n\n\nDefine the following files in your overlay directory. Note that we utilize distinct patch files for each component to ensure explicit targeting and avoid accidental modification of infrastructure components (such as the database).\n\n\nTarget: Application Deployments (Decision Center, Runner, Console, Runtime).\nPurpose: Injects the mandatory Group IDs required by the “Restricted” OPA policy.\nDownload security-patch.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: .*\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsUser: 1001\n        runAsGroup: 1001\n        supplementalGroups: [1001]\n\n\n\nTarget: The Database Connection Test Job.\nPurpose: This job is “unconfigurable” in the standard chart. We must patch it to enforce Image Digests, Resource Limits, and strict Security Contexts.\nDownload job-security-patch.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: .*\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-lab-odm-test: runtime/default  \n    spec:\n      # Pod Level Security\n      securityContext:\n        runAsUser: 1001\n        runAsGroup: 1001\n        supplementalGroups: [1001]\n        seccompProfile:\n          type: RuntimeDefault\n\n      containers:\n      - name: odm-lab-odm-test\n        # CRITICAL: The Helm chart does not support digests for this specific job.\n        # You must hardcode the mirrored Runtime image and SHA256 digest here or comment out to use default cp.icr.io\n        image: artifactory.internal.corp/odm-repo/odm-decisionserverruntime@sha256:ab03e4e35923c674a090456f6869963a6d29e8f94117061ff11d383cc8c9369a\n        # Use the following for default image from IBM Container Registry\n        #image: cp.icr.io/cp/cp4a/odm/odm-decisionserverruntime@sha256:ab03e4e35923c674a090456f6869963a6d29e8f94117061ff11d383cc8c9369a\n\n        # Fix \"container-must-have-limits-and-requests\"\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n\n        # Fix \"privilege-escalation\", \"capabilities\", \"readonlyrootfilesystem\"\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n\n\n\nTarget: Decision Center Deployment.\nPurpose: Injects the mandatory runAsGroup: 1001 and supplementalGroups into the Decision Center pods, as the Helm chart template ignores these values for this component.\nDownload security-patch-dc.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisioncenter\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisioncenter: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisioncenter: runtime/default\n\n\n\nTarget: Decision Runner Deployment.\nPurpose: Injects the mandatory runAsGroup: 1001 and supplementalGroups into the Decision Runner pods to satisfy the “Restricted” OPA policy.\nDownload security-patch-runner.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionrunner\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionrunner: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionrunner: runtime/default\n\n\n\nTarget: Decision Server Console Deployment.\nPurpose: Injects the mandatory runAsGroup: 1001 and supplementalGroups into the Rule Execution Server Console pods.\nDownload security-patch-console.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionserverconsole\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionserverconsole: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionserverconsole: runtime/default\n\n\n\nTarget: Decision Server Runtime Deployment.\nPurpose: Injects the mandatory runAsGroup: 1001 and supplementalGroups into the Rule Execution Server Runtime pods.\nDownload security-patch-runtime.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-lab-odm-decisionserverruntime\nspec:\n  template:\n    metadata:\n      annotations:\n        container.apparmor.security.beta.kubernetes.io/odm-decisionserverruntime: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-folder-readonlyfs: runtime/default\n        container.apparmor.security.beta.kubernetes.io/init-decisionserverruntime: runtime/default\n\n\n\nTarget: All ODM Deployments and Jobs.\nPurpose: A JSON Patch that overrides the imagePullPolicy for all containers (including Init Containers, which cannot be configured via Helm). This forces the policy to Always, satisfying strict image currency requirements.\nDownload force-pull-policy.yaml\n# force-pull-policy.yaml\n# Forces Main Container\n- op: replace\n  path: /spec/template/spec/containers/0/imagePullPolicy\n  value: Always\n# Forces First Init Container (e.g. init-folder-readonlyfs)\n- op: replace\n  path: /spec/template/spec/initContainers/0/imagePullPolicy\n  value: Always\n# Forces Second Init Container (e.g. init-decisionrunner)\n# Note: This needs uncommented if using internal database in a lab setting for OPA constraint\n# - op: replace\n#   path: /spec/template/spec/initContainers/1/imagePullPolicy\n#   value: Always\n\n\n\n\nCreate the kustomization.yaml file to link the patches to the generated resources.\nImportant: We use explicit name targeting to ensure we do not accidentally patch the Database Deployment (if running locally) or other infrastructure components.\nNote: The names below assume your Helm Release Name is odm-lab. If you use odm-pilot, update the names accordingly (e.g., odm-pilot-odm-decisioncenter).\nDownload kustomization.yaml\nresources:\n  - odm-raw.yaml\n\npatches:\n  # Generic Security Patch (Group IDs + Token)\n  # Targets ALL deployments via regex name\n  - path: security-patch.yaml\n    target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: \"odm-lab-odm-.*\" # \"odm-pilot-odm-.*\"\n\n  # Specific AppArmor Patches (One per file)\n  - path: security-patch-dc.yaml\n  - path: security-patch-runner.yaml\n  - path: security-patch-console.yaml\n  - path: security-patch-runtime.yaml\n\n  # Test Job Patch\n  - path: job-security-patch.yaml\n    target:\n      group: batch\n      version: v1\n      kind: Job\n      name: odm-lab-odm-test # \"odm-pilot-odm-test\"\n\n  # Force Pull Policy (JSON Patch)\n  # Applies to all ODM deployments\n  - path: force-pull-policy.yaml\n    target:\n      group: apps\n      version: v1\n      kind: Deployment\n      name: \"odm-lab-odm-.*\" # \"odm-pilot-odm-.*\"\n\n\n\n\n\n\nTipLab Workaround: Non-AppArmor Hosts (RHEL/CentOS/Rocky)\n\n\n\nIf you are running this deployment in a lab environment based on RHEL, CentOS, or Rocky Linux, your kernel likely uses SELinux instead of AppArmor.\nIncluding the container.apparmor.security.beta.kubernetes.io annotations (which are mandatory for the restricted EKS environment) will cause your lab pods to hang in a Blocked state with the error: Cannot enforce AppArmor: AppArmor is not enabled on the host.\nAction: Run the following command to strip these annotations from all YAML files in your current directory before applying the configuration:\nsed -i '/container.apparmor.security.beta.kubernetes.io/d' *.yaml\n\n\n\n\n\n\nRun the build pipeline to generate the patched manifests and apply them to the cluster.\n# 1. Render Helm Template\n# (Change values-prod.yaml to values-lab.yaml if needed)\nhelm template odm-lab ibm-helm/ibm-odm-prod \\\n  --version 25.1.0 \\\n  --kube-version 1.28.0 \\\n  -f values-prod.yaml &gt; odm-raw.yaml\n\n# 2. Apply Patches & Deploy\nkubectl -n odm-pilot apply -k .\n\n\n\n\n\n\nTipVerification\n\n\n\nAfter deployment, verify that the ALB has successfully registered the targets.\nkubectl get ingress -n odm-pilot\n# Look for the ADDRESS field (e.g., k8s-odmpilot-xxxx.us-east-1.elb.amazonaws.com)\nNavigate to https://odm.mycompany.com/decisioncenter. If the page loads securely, End-to-End encryption is functioning correctly.\nUse odmAdmin and the password you set in the values.yaml to log in (e.g. odmAdminPassword123!).",
    "crumbs": [
      "Implementation Methodology",
      "ODM Deployment"
    ]
  },
  {
    "objectID": "src/implementation_methodology/stepone-imp.html",
    "href": "src/implementation_methodology/stepone-imp.html",
    "title": "Phase 1: Foundation & Prerequisite Configuration",
    "section": "",
    "text": "Before deploying the ODM application logic, the infrastructure foundation must be secured. This phase covers provisioning the database, generating internal TLS assets for end-to-end encryption, and creating the necessary Kubernetes secrets.\n\n\nCreate the dedicated namespace for the ODM deployment. It is recommended to set your current context to this namespace to simplify subsequent commands.\n# 1. Create the namespace\nkubectl create namespace odm-pilot\n\n# 2. Set as current context (Optional but recommended)\nkubectl config set-context --current --namespace=odm-pilot\n\n\n\n\nOption A: OracleOption B: Lab Manual Deployment (PostgreSQL)Option C: Lab Jenkins Deployment (PostgreSQL)Option D: Lab (Oracle Free)\n\n\nODM requires a robust persistence layer. We utilize Oracle on EC2 (19c or 23ai).\n\nProvision Oracle: Ensure the instance is deployed in private subnets reachable by the EKS cluster.\nConfigure Security Groups:\n\nInbound Rule: Allow TCP/1521 from the EKS Cluster Security Group.\nOutbound Rule: Allow return traffic.\n\n\n\n\nCreate PostgreSQL in a separate name space to similate a similar setup to a prod environment with AWS RDS.\n\nCreate New Namespace (PostgreSQL) From a node inside your cluster on Bastion (eg. [clouduser@my-k3s-server-0 ~])\nkubectl create namespace postgres\nNote: You can check namespaces with bash kubectl get ns\nCreate PostgreSQL Secret\nkubectl -n postgres create secret generic postgres-secret \\\n  --from-literal=POSTGRES_DB=postgres \\\n  --from-literal=POSTGRES_USER=postgres \\\n  --from-literal=POSTGRES_PASSWORD='StrongPassword123!'\nCreate persistent value storage\nmkdir -p ~/k3s/postgres\ncd ~/k3s/postgres\nvi postgres-pvc.yaml\nPaste into postgres-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc\n  namespace: postgres\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\nSave the file and apply it.\nkubectl apply -f postgres-pvc.yaml\nShould output: persistentvolumeclaim/postgres-pvc created\nCreate the deployment yaml\nvi postgres-deployment.yaml\nPaste the following into the yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  namespace: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:16\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\nSave the file and apply it.\nkubectl apply -f postgres-deployment.yaml\nExpected output: deployment.apps/postgres created\nCheck that the Postgres pod is running\nkubectl -n postgres get pods -w\nExpect one of the following:\npostgres-xxxxx   Pending\npostgres-xxxxx   ContainerCreating\npostgres-xxxxx   Running\nEventually you should see ‘Running’\nCheck that the volume is mounted\nkubectl -n postgres get pvc\nNote: if it gets stuck, run kubectl -n postgres describe pod postgres-xxxxx\nCreate the service file (once pod is running)\nvi postgres-service.yaml\nPaste\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\n  namespace: postgres\nspec:\n  type: ClusterIP\n  selector:\n    app: postgres\n  ports:\n  - port: 5432\n    targetPort: 5432\nApply the yaml\nkubectl apply -f postgres-service.yaml\nExpected output service/postgres created\nVerify that the volume is running\nkubectl -n postgres get svc\nYou should now have postgres-pvc.yaml, postgres-deployment.yaml, and postgres-service.yaml under the ~/k3s/postgres directory.\nConnect to the database to create schema and user\nkubectl exec -it -n postgres \\\n$(kubectl get pods -n postgres -l app=postgres --field-selector=status.phase=Running -o jsonpath=\"{.items[0].metadata.name}\") \\\n-- psql -U postgres\n\n\n\n\n\n\n\nNoteDatabase Connection Details:\n\n\n\nThe database host for the internal PostgreSQL deployment will be postgres.postgres.svc.cluster.local on port 5432. Use this for configuring the ODM application connection string in the subsequent steps.\n\n\n\n\n\nSet up Environment Variables & Check Cluster in Jenkins Pipeline\nSet KUBECTL, NAMESPACE, HELM_RELEASE, and HELM.\nenvironment {\n  KUBECTL = '/var/tmp/kubectl'\n  NAMESPACE = 'odm-jenkins'\n  HELM_RELEASE = 'odm-lab'\n  HELM    = '/var/tmp/helm'\n}\nCheck Cluster Connection\nstages {\n\n  stage('Check Cluster') {\n    steps {\n      script {\n        sh \"chmod u+x ${KUBECTL}\"\n        withKubeConfig(credentialsId: 'k3s-kubeconfig') {\n          sh \"${KUBECTL} get nodes\"\n        }\n      }\n    }\n  }\n}\nCreate New Namespace (PostgreSQL)\nAdd Stage to Jenkins file under Check Cluster Stage\nstage('Create Namespace') {\n  steps {\n    script {\n      withKubeConfig(credentialsId: 'k3s-kubeconfig') {\n        sh \"\"\"\n        ${KUBECTL} get namespace ${NAMESPACE} &gt;/dev/null 2&gt;&1 || \\\n        ${KUBECTL} create namespace ${NAMESPACE}\n        \"\"\"\n      }\n    }\n  }\n}\nCreate Postgres ODM Secret in Jenkins\nInside your Jenkins Deployment, go to Manage Jenkins\\(\\rightarrow\\)Credentials\\(\\rightarrow\\)Global\\(\\rightarrow\\)Add Credentials\nKind: Username with password\nScope: Global\nUsername: admin\nPassword: admin123\nID: odm-db-credentials\nSelect Create\nCreate Postgres ODM Secret in Jenkins\nAdd Stage to Jenkins file under Create Namespace\n  stage('Deploy Postgres (Lab)') {\n    steps {\n      script {\n        withCredentials([usernamePassword(\n          credentialsId: 'odm-db-credentials',\n            usernameVariable: 'DB_USER',\n              passwordVariable: 'DB_PASS'\n        )]) {\n          withKubeConfig(credentialsId: 'k3s-kubeconfig') {\n            sh \"\"\"\necho \"Creating Postgres secret...\"\n\n${KUBECTL} -n ${NAMESPACE} create secret generic postgres-secret \\\n  --from-literal=POSTGRES_DB=odmdb \\\n  --from-literal=POSTGRES_USER=${DB_USER} \\\n  --from-literal=POSTGRES_PASSWORD=${DB_PASS} \\\n  --dry-run=client -o yaml | ${KUBECTL} apply -f -\n\necho \"Deploying Postgres securely...\"\n\ncat &lt;&lt;EOF | ${KUBECTL} -n ${NAMESPACE} apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: odm-postgres\n  template:\n    metadata:\n      labels:\n        app: odm-postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:13\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - containerPort: 5432\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: odm-postgres\nspec:\n  selector:\n    app: odm-postgres\n  ports:\n  - port: 5432\n    targetPort: 5432\nEOF\n\n${KUBECTL} -n ${NAMESPACE} rollout status deployment/odm-postgres --timeout=180s\n\"\"\"\n        }\n      }\n    }\n  }\n}\n\n\n\nHere are instructions to deploy Oracle Database 23ai Free into a dedicated oracle namespace in a k3s lab.\n\nCreate the Deployment Manifest Create a file named oracle-lab.yaml.\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: oracle\n  labels:\n    # Optional: Label to help with OPA exclusion if needed later\n    name: oracle\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: oracle-data\n  namespace: oracle\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: local-path\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: oracle-db\n  namespace: oracle\n  labels:\n    app: oracle-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: oracle-db\n  template:\n    metadata:\n      labels:\n        app: oracle-db\n    spec:\n      # Oracle 23ai runs as UID 54321 by default\n      securityContext:\n        fsGroup: 54321\n        runAsUser: 54321\n        runAsGroup: 54321\n      containers:\n      - name: oracle\n        image: container-registry.oracle.com/database/free:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 1521\n        env:\n        # The password for SYS, SYSTEM, and PDBADMIN\n        - name: ORACLE_PWD\n          value: \"StrongPassword123\"\n        volumeMounts:\n        - name: oracle-data\n          mountPath: /opt/oracle/oradata\n        resources:\n          requests:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2000m\"\n        # Health check to ensure DB is open before ODM tries to connect\n        startupProbe:\n          exec:\n            command: [\"/opt/oracle/checkDBStatus.sh\"]\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          failureThreshold: 40 # Wait up to 10 mins for first boot\n      volumes:\n      - name: oracle-data\n        persistentVolumeClaim:\n          claimName: oracle-data\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: oracle-db\n  namespace: oracle\nspec:\n  selector:\n    app: oracle-db\n  ports:\n    - protocol: TCP\n      port: 1521\n      targetPort: 1521\n\nDeploy to K3s\n\nRun the apply command:\nkubectl apply -f oracle-lab.yaml\nMonitor the Startup: Oracle takes a while (5-10 minutes) to initialize the database files on the first run. Watch the logs until you see DATABASE IS READY TO USE!.\nkubectl logs -f -n oracle -l app=oracle-db\n\n\n\n\n\n\nNoteDatabase Connection Details:\n\n\n\nThe database connection url for the internal Oracle deployment will be jdbc:oracle:thin:@//oracle-db.oracle.svc.cluster.local:1521/freepdb1.\n\n\n\n\n\n\n\n\nThe ODM data source requires specific privileges to initialize the schema on the first startup. Connect to your RDS instance via a bastion host or temporary pod and execute the following SQL commands:\n\nOption A (Oracle)Option B (Postgres)\n\n\nA. (Lab ONLY) Log into the Container via SQL*Plus: We connect as SYS (Superuser) to the default Pluggable Database (FREEPDB1).\n# Get the pod name\nexport ORACLE_POD=$(kubectl get pod -n oracle -l app=oracle-db -o jsonpath=\"{.items[0].metadata.name}\")\n\n# Exec into SQL*Plus\n# Note: Connection string is host:port/ServiceName\nkubectl exec -it -n oracle $ORACLE_POD -- sqlplus sys/StrongPassword123@//localhost:1521/FREEPDB1 as sysdba\nB. Run the Setup SQL: Paste this block directly into the SQL&gt; prompt:\n\n\n\n\n\n\nNoteUsername and password\n\n\n\nReplace ODM_USER and StrongPassword123 with the username and password of your choice.\n\n\n-- 1. Create User/Schema\nCREATE USER ODM_USER IDENTIFIED BY \"StrongPassword123\" DEFAULT TABLESPACE USERS QUOTA UNLIMITED ON USERS;\n\n-- 2. Grant Schema Privileges\nGRANT CREATE SESSION TO ODM_USER;\nGRANT CREATE TABLE TO ODM_USER;\nGRANT CREATE VIEW TO ODM_USER;\nGRANT CREATE SEQUENCE TO ODM_USER;\nGRANT CREATE TRIGGER TO ODM_USER;\n\n-- 3. Grant XA Recovery Privileges (Required for ODM/Liberty)\nGRANT SELECT ON sys.dba_pending_transactions TO ODM_USER;\nGRANT SELECT ON sys.pending_trans$ TO ODM_USER;\nGRANT SELECT ON sys.dba_2pc_pending TO ODM_USER;\nGRANT EXECUTE ON sys.dbms_xa TO ODM_USER;\n\n-- 4. Verify\nSELECT username FROM dba_users WHERE username = 'ODM_USER';\n\n-- 5. Exit\nexit;\n\n\n-- 1. Create the dedicated ODM user\nCREATE USER odm WITH PASSWORD 'StrongPassword123!';\n\n-- 2. Create the database\nCREATE DATABASE odm_db OWNER odm;\n\n-- 3. Grant privileges (Required for table creation)\nGRANT ALL PRIVILEGES ON DATABASE odm_db TO odm;\n\n-- 4. (Optional) If using a specific schema\n\\c odm_db\nCREATE SCHEMA odm_rules AUTHORIZATION odm;\n\n\n\n\n\n\nTo satisfy the OPA policy requiring HTTPS traffic at the cluster boundary, the Kubernetes Ingress resource must be configured with a valid TLS secret. This enables the Ingress Controller to terminate HTTPS traffic at the cluster boundary.\nFor this document, we will generate a self-signed certificate using OpenSSL.\nGenerate the Certificate and Key (PEM):\n\n\n\n\n\n\nNoteDetermining the Certificate Subject (CN)\n\n\n\nThe Common Name (/CN) in the certificate must match the exact Fully Qualified Domain Name (FQDN) that users will type into their browser.\n\nIn the Lab: We use the pattern odm.&lt;proxy&gt; (e.g., /CN=odm.my-haproxy.gym.lan/O=Lab/C=US). This ensures the browser accepts the certificate when traffic is routed through your lab’s load balancer.\nIn Production (AWS ALB): Use the Route53 CNAME or Alias record created for the application (e.g., /CN=odm.internal.corp).\n\nImportant: Do not use the raw AWS ALB hostname (e.g., *.elb.amazonaws.com) as the CN. Browser security policies will reject the certificate if it identifies the load balancer hardware rather than the application service name.\n\n\n\n\n# 1. Generate a self-signed certificate and private key\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\\n  -keyout odm-lab.key \\\n  -out odm-lab.crt \\\n  -subj \"/CN=odm.internal.corp/O=MyCorp/C=US\"\n\n# 2. Verify the content\nls -l odm-lab.key odm-lab.crt\n\n\n\nWith the database credentials defined and the keystore generated, inject them into the cluster as Kubernetes Secrets.\nDatabase Credentials Secret:\nThe ODM application requires a Kubernetes Secret to authenticate with the database. Choose the option matching your deployment strategy.\n\nOption A: Oracle (External)Option B: Lab (Internal DB)Option C: Lab (Postgres)\n\n\nTarget: Pilot environment using external Oracle.\nCreate a secret containing the credentials for your external Oracle instance.\nkubectl create secret generic odm-db-secret \\\n  --namespace odm-pilot \\\n  --from-literal=db-user='ODM_USER' \\\n  --from-literal=db-password='StrongPassword123' \\\n  --from-literal=db-name=freepdb1 \\\n  --from-literal=db-server=oracle.cxxxxx.us-east-1.rds.amazonaws.com\n# for lab Oracle use `oracle-db.oracle.svc.cluster.local` as db-server\nNote: Ensure the secret name (odm-db-secret) matches the secretCredentials field in your values-prod.yaml.\n\n\nTarget: Sandbox / Local Lab using the internal containerized database.\nCreate a simple secret for the internal PostgreSQL container.\nkubectl create secret generic odm-db-secret \\\n  --namespace odm-pilot \\\n  --from-literal=db-user=odm \\\n  --from-literal=db-password='StrongPassword123!'\nNote: Ensure the secret name (odm-db-secret) matches the secretCredentials field in your values-lab.yaml.\n\n\nTarget: Sandbox / Local Lab using the “external” postgres database.\nCreate a simple secret for the PostgreSQL database running in the postgres namespace.\nkubectl create secret generic odm-db-secret \\\n  --namespace odm-pilot \\\n  --from-literal=db-user='odm' \\\n  --from-literal=db-password='StrongPassword123!' \\\n  --from-literal=db-server='postgres.postgres.svc.cluster.local'\nNote: Ensure the secret name (odm-db-secret) matches the secretCredentials field in your values-lab.yaml.\n\n\n\nTLS Keystore Secret: This will be referenced by the Ingress resource (tlsSecretRef) to enable HTTPS.\n# Create a standard Kubernetes TLS secret type\nkubectl create secret tls odm-tls-secret \\\n  --namespace odm-pilot \\\n  --key odm-lab.key \\\n  --cert odm-lab.crt\n\n\n\n\n\n\nNoteSecret Management\n\n\n\nIn a production environment, avoid creating secrets from literals in the CLI history. Use an External Secrets Operator (ESO) to sync these values from AWS Secrets Manager or HashiCorp Vault.\n\n\n\n\n\nKubernetes requires authentication credentials to pull container images. Depending on your environment constraints (Lab vs. Restricted Production), the source registry and credentials will differ.\n\nOption A: Private Registry (Production/Restricted)Option B: IBM Registry (Standard Lab)Option C: Lab Jenkins Deployment (PostgreSQL)\n\n\nTarget Environment: Customer Pilot / Air-Gapped / OPA-Enforced\nIn strict environments where public internet access is blocked or OPA forbids public registries, you must pull from the internal location where you mirrored the images (e.g., Artifactory).\nAction: Create a secret using your internal registry credentials.\n# Replace with your internal registry details\nkubectl create secret docker-registry internal-registry-secret \\\n  --docker-server=artifactory.internal.corp:8443 \\\n  --docker-username=&lt;SERVICE_ACCOUNT_USER&gt; \\\n  --docker-password=&lt;SERVICE_ACCOUNT_TOKEN&gt; \\\n  --docker-email=admin@internal.corp \\\n  -n odm-pilot\n\n\nTarget Environment: Sandbox / POC with Internet Access\nIf you are working in a lab with direct internet access and no strict OPA registry constraints, you can pull directly from IBM.\nAction: Create a secret using your IBM Entitlement Key.\n# 1. Get your key from myibm.ibm.com/products-services/containerlibrary\n# 2. Create the secret\nkubectl create secret docker-registry internal-registry-secret \\\n  --docker-server=cp.icr.io \\\n  --docker-username=cp \\\n  --docker-password=&lt;YOUR_IBM_ENTITLEMENT_KEY&gt; \\\n  --docker-email=user@example.com \\\n  -n odm-pilot\n\n\nInside your Jenkins Deployment, navigate to:\nManage Jenkins → Credentials → Global → Add Credentials\nFill in the fields below:\n\n\n\nField\nValue\n\n\n\n\nKind\nSecret Text\n\n\nScope\nGlobal\n\n\nSecret\nYOUR_IBM_ENTITLEMENT_KEY\n\n\nID\nicr-entitlement-key\n\n\n\nClick Create.\nAdd the following stage after the Deploy Postgres (Lab) stage\nstage('Create ICR Pull Secret') {\n  steps {\n    script {\n      withCredentials([\n        string(credentialsId: 'icr-entitlement-key', variable: 'ICR_KEY')\n      ]) {\n        withKubeConfig(credentialsId: 'k3s-kubeconfig') {\n          sh '''\necho \"Creating docker-registry secret for cp.icr.io...\"\n\nNAMESPACE=\"default\"\nSECRET_NAME=\"icr-secret\"\n\n${KUBECTL} -n ${NAMESPACE} create secret docker-registry ${SECRET_NAME} \\\n  --docker-server=cp.icr.io \\\n  --docker-username=cp \\\n  --docker-password=${ICR_KEY} \\\n  --docker-email=dummy@example.com \\\n  --dry-run=client -o yaml | ${KUBECTL} apply -f -\n'''\n        }\n      }\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\nImportantSecret Name Consistency\n\n\n\nWhichever option you choose, ensure the secret name used in the kubectl create command exactly matches the value in your values.yaml file under image.pullSecrets.",
    "crumbs": [
      "Implementation Methodology",
      "Foundation & Prereqs"
    ]
  },
  {
    "objectID": "src/implementation_methodology/stepone-imp.html#phase-1-foundation-prerequisite-configuration",
    "href": "src/implementation_methodology/stepone-imp.html#phase-1-foundation-prerequisite-configuration",
    "title": "Phase 1: Foundation & Prerequisite Configuration",
    "section": "",
    "text": "Before deploying the ODM application logic, the infrastructure foundation must be secured. This phase covers provisioning the database, generating internal TLS assets for end-to-end encryption, and creating the necessary Kubernetes secrets.\n\n\nCreate the dedicated namespace for the ODM deployment. It is recommended to set your current context to this namespace to simplify subsequent commands.\n# 1. Create the namespace\nkubectl create namespace odm-pilot\n\n# 2. Set as current context (Optional but recommended)\nkubectl config set-context --current --namespace=odm-pilot\n\n\n\n\nOption A: OracleOption B: Lab Manual Deployment (PostgreSQL)Option C: Lab Jenkins Deployment (PostgreSQL)Option D: Lab (Oracle Free)\n\n\nODM requires a robust persistence layer. We utilize Oracle on EC2 (19c or 23ai).\n\nProvision Oracle: Ensure the instance is deployed in private subnets reachable by the EKS cluster.\nConfigure Security Groups:\n\nInbound Rule: Allow TCP/1521 from the EKS Cluster Security Group.\nOutbound Rule: Allow return traffic.\n\n\n\n\nCreate PostgreSQL in a separate name space to similate a similar setup to a prod environment with AWS RDS.\n\nCreate New Namespace (PostgreSQL) From a node inside your cluster on Bastion (eg. [clouduser@my-k3s-server-0 ~])\nkubectl create namespace postgres\nNote: You can check namespaces with bash kubectl get ns\nCreate PostgreSQL Secret\nkubectl -n postgres create secret generic postgres-secret \\\n  --from-literal=POSTGRES_DB=postgres \\\n  --from-literal=POSTGRES_USER=postgres \\\n  --from-literal=POSTGRES_PASSWORD='StrongPassword123!'\nCreate persistent value storage\nmkdir -p ~/k3s/postgres\ncd ~/k3s/postgres\nvi postgres-pvc.yaml\nPaste into postgres-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc\n  namespace: postgres\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\nSave the file and apply it.\nkubectl apply -f postgres-pvc.yaml\nShould output: persistentvolumeclaim/postgres-pvc created\nCreate the deployment yaml\nvi postgres-deployment.yaml\nPaste the following into the yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: postgres\n  namespace: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:16\n        ports:\n        - containerPort: 5432\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n      volumes:\n      - name: postgres-storage\n        persistentVolumeClaim:\n          claimName: postgres-pvc\nSave the file and apply it.\nkubectl apply -f postgres-deployment.yaml\nExpected output: deployment.apps/postgres created\nCheck that the Postgres pod is running\nkubectl -n postgres get pods -w\nExpect one of the following:\npostgres-xxxxx   Pending\npostgres-xxxxx   ContainerCreating\npostgres-xxxxx   Running\nEventually you should see ‘Running’\nCheck that the volume is mounted\nkubectl -n postgres get pvc\nNote: if it gets stuck, run kubectl -n postgres describe pod postgres-xxxxx\nCreate the service file (once pod is running)\nvi postgres-service.yaml\nPaste\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres\n  namespace: postgres\nspec:\n  type: ClusterIP\n  selector:\n    app: postgres\n  ports:\n  - port: 5432\n    targetPort: 5432\nApply the yaml\nkubectl apply -f postgres-service.yaml\nExpected output service/postgres created\nVerify that the volume is running\nkubectl -n postgres get svc\nYou should now have postgres-pvc.yaml, postgres-deployment.yaml, and postgres-service.yaml under the ~/k3s/postgres directory.\nConnect to the database to create schema and user\nkubectl exec -it -n postgres \\\n$(kubectl get pods -n postgres -l app=postgres --field-selector=status.phase=Running -o jsonpath=\"{.items[0].metadata.name}\") \\\n-- psql -U postgres\n\n\n\n\n\n\n\nNoteDatabase Connection Details:\n\n\n\nThe database host for the internal PostgreSQL deployment will be postgres.postgres.svc.cluster.local on port 5432. Use this for configuring the ODM application connection string in the subsequent steps.\n\n\n\n\n\nSet up Environment Variables & Check Cluster in Jenkins Pipeline\nSet KUBECTL, NAMESPACE, HELM_RELEASE, and HELM.\nenvironment {\n  KUBECTL = '/var/tmp/kubectl'\n  NAMESPACE = 'odm-jenkins'\n  HELM_RELEASE = 'odm-lab'\n  HELM    = '/var/tmp/helm'\n}\nCheck Cluster Connection\nstages {\n\n  stage('Check Cluster') {\n    steps {\n      script {\n        sh \"chmod u+x ${KUBECTL}\"\n        withKubeConfig(credentialsId: 'k3s-kubeconfig') {\n          sh \"${KUBECTL} get nodes\"\n        }\n      }\n    }\n  }\n}\nCreate New Namespace (PostgreSQL)\nAdd Stage to Jenkins file under Check Cluster Stage\nstage('Create Namespace') {\n  steps {\n    script {\n      withKubeConfig(credentialsId: 'k3s-kubeconfig') {\n        sh \"\"\"\n        ${KUBECTL} get namespace ${NAMESPACE} &gt;/dev/null 2&gt;&1 || \\\n        ${KUBECTL} create namespace ${NAMESPACE}\n        \"\"\"\n      }\n    }\n  }\n}\nCreate Postgres ODM Secret in Jenkins\nInside your Jenkins Deployment, go to Manage Jenkins\\(\\rightarrow\\)Credentials\\(\\rightarrow\\)Global\\(\\rightarrow\\)Add Credentials\nKind: Username with password\nScope: Global\nUsername: admin\nPassword: admin123\nID: odm-db-credentials\nSelect Create\nCreate Postgres ODM Secret in Jenkins\nAdd Stage to Jenkins file under Create Namespace\n  stage('Deploy Postgres (Lab)') {\n    steps {\n      script {\n        withCredentials([usernamePassword(\n          credentialsId: 'odm-db-credentials',\n            usernameVariable: 'DB_USER',\n              passwordVariable: 'DB_PASS'\n        )]) {\n          withKubeConfig(credentialsId: 'k3s-kubeconfig') {\n            sh \"\"\"\necho \"Creating Postgres secret...\"\n\n${KUBECTL} -n ${NAMESPACE} create secret generic postgres-secret \\\n  --from-literal=POSTGRES_DB=odmdb \\\n  --from-literal=POSTGRES_USER=${DB_USER} \\\n  --from-literal=POSTGRES_PASSWORD=${DB_PASS} \\\n  --dry-run=client -o yaml | ${KUBECTL} apply -f -\n\necho \"Deploying Postgres securely...\"\n\ncat &lt;&lt;EOF | ${KUBECTL} -n ${NAMESPACE} apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: odm-postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: odm-postgres\n  template:\n    metadata:\n      labels:\n        app: odm-postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:13\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - containerPort: 5432\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: odm-postgres\nspec:\n  selector:\n    app: odm-postgres\n  ports:\n  - port: 5432\n    targetPort: 5432\nEOF\n\n${KUBECTL} -n ${NAMESPACE} rollout status deployment/odm-postgres --timeout=180s\n\"\"\"\n        }\n      }\n    }\n  }\n}\n\n\n\nHere are instructions to deploy Oracle Database 23ai Free into a dedicated oracle namespace in a k3s lab.\n\nCreate the Deployment Manifest Create a file named oracle-lab.yaml.\n\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: oracle\n  labels:\n    # Optional: Label to help with OPA exclusion if needed later\n    name: oracle\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: oracle-data\n  namespace: oracle\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n  storageClassName: local-path\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: oracle-db\n  namespace: oracle\n  labels:\n    app: oracle-db\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: oracle-db\n  template:\n    metadata:\n      labels:\n        app: oracle-db\n    spec:\n      # Oracle 23ai runs as UID 54321 by default\n      securityContext:\n        fsGroup: 54321\n        runAsUser: 54321\n        runAsGroup: 54321\n      containers:\n      - name: oracle\n        image: container-registry.oracle.com/database/free:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 1521\n        env:\n        # The password for SYS, SYSTEM, and PDBADMIN\n        - name: ORACLE_PWD\n          value: \"StrongPassword123\"\n        volumeMounts:\n        - name: oracle-data\n          mountPath: /opt/oracle/oradata\n        resources:\n          requests:\n            memory: \"2Gi\"\n            cpu: \"1000m\"\n          limits:\n            memory: \"4Gi\"\n            cpu: \"2000m\"\n        # Health check to ensure DB is open before ODM tries to connect\n        startupProbe:\n          exec:\n            command: [\"/opt/oracle/checkDBStatus.sh\"]\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          failureThreshold: 40 # Wait up to 10 mins for first boot\n      volumes:\n      - name: oracle-data\n        persistentVolumeClaim:\n          claimName: oracle-data\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: oracle-db\n  namespace: oracle\nspec:\n  selector:\n    app: oracle-db\n  ports:\n    - protocol: TCP\n      port: 1521\n      targetPort: 1521\n\nDeploy to K3s\n\nRun the apply command:\nkubectl apply -f oracle-lab.yaml\nMonitor the Startup: Oracle takes a while (5-10 minutes) to initialize the database files on the first run. Watch the logs until you see DATABASE IS READY TO USE!.\nkubectl logs -f -n oracle -l app=oracle-db\n\n\n\n\n\n\nNoteDatabase Connection Details:\n\n\n\nThe database connection url for the internal Oracle deployment will be jdbc:oracle:thin:@//oracle-db.oracle.svc.cluster.local:1521/freepdb1.\n\n\n\n\n\n\n\n\nThe ODM data source requires specific privileges to initialize the schema on the first startup. Connect to your RDS instance via a bastion host or temporary pod and execute the following SQL commands:\n\nOption A (Oracle)Option B (Postgres)\n\n\nA. (Lab ONLY) Log into the Container via SQL*Plus: We connect as SYS (Superuser) to the default Pluggable Database (FREEPDB1).\n# Get the pod name\nexport ORACLE_POD=$(kubectl get pod -n oracle -l app=oracle-db -o jsonpath=\"{.items[0].metadata.name}\")\n\n# Exec into SQL*Plus\n# Note: Connection string is host:port/ServiceName\nkubectl exec -it -n oracle $ORACLE_POD -- sqlplus sys/StrongPassword123@//localhost:1521/FREEPDB1 as sysdba\nB. Run the Setup SQL: Paste this block directly into the SQL&gt; prompt:\n\n\n\n\n\n\nNoteUsername and password\n\n\n\nReplace ODM_USER and StrongPassword123 with the username and password of your choice.\n\n\n-- 1. Create User/Schema\nCREATE USER ODM_USER IDENTIFIED BY \"StrongPassword123\" DEFAULT TABLESPACE USERS QUOTA UNLIMITED ON USERS;\n\n-- 2. Grant Schema Privileges\nGRANT CREATE SESSION TO ODM_USER;\nGRANT CREATE TABLE TO ODM_USER;\nGRANT CREATE VIEW TO ODM_USER;\nGRANT CREATE SEQUENCE TO ODM_USER;\nGRANT CREATE TRIGGER TO ODM_USER;\n\n-- 3. Grant XA Recovery Privileges (Required for ODM/Liberty)\nGRANT SELECT ON sys.dba_pending_transactions TO ODM_USER;\nGRANT SELECT ON sys.pending_trans$ TO ODM_USER;\nGRANT SELECT ON sys.dba_2pc_pending TO ODM_USER;\nGRANT EXECUTE ON sys.dbms_xa TO ODM_USER;\n\n-- 4. Verify\nSELECT username FROM dba_users WHERE username = 'ODM_USER';\n\n-- 5. Exit\nexit;\n\n\n-- 1. Create the dedicated ODM user\nCREATE USER odm WITH PASSWORD 'StrongPassword123!';\n\n-- 2. Create the database\nCREATE DATABASE odm_db OWNER odm;\n\n-- 3. Grant privileges (Required for table creation)\nGRANT ALL PRIVILEGES ON DATABASE odm_db TO odm;\n\n-- 4. (Optional) If using a specific schema\n\\c odm_db\nCREATE SCHEMA odm_rules AUTHORIZATION odm;\n\n\n\n\n\n\nTo satisfy the OPA policy requiring HTTPS traffic at the cluster boundary, the Kubernetes Ingress resource must be configured with a valid TLS secret. This enables the Ingress Controller to terminate HTTPS traffic at the cluster boundary.\nFor this document, we will generate a self-signed certificate using OpenSSL.\nGenerate the Certificate and Key (PEM):\n\n\n\n\n\n\nNoteDetermining the Certificate Subject (CN)\n\n\n\nThe Common Name (/CN) in the certificate must match the exact Fully Qualified Domain Name (FQDN) that users will type into their browser.\n\nIn the Lab: We use the pattern odm.&lt;proxy&gt; (e.g., /CN=odm.my-haproxy.gym.lan/O=Lab/C=US). This ensures the browser accepts the certificate when traffic is routed through your lab’s load balancer.\nIn Production (AWS ALB): Use the Route53 CNAME or Alias record created for the application (e.g., /CN=odm.internal.corp).\n\nImportant: Do not use the raw AWS ALB hostname (e.g., *.elb.amazonaws.com) as the CN. Browser security policies will reject the certificate if it identifies the load balancer hardware rather than the application service name.\n\n\n\n\n# 1. Generate a self-signed certificate and private key\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 \\\n  -keyout odm-lab.key \\\n  -out odm-lab.crt \\\n  -subj \"/CN=odm.internal.corp/O=MyCorp/C=US\"\n\n# 2. Verify the content\nls -l odm-lab.key odm-lab.crt\n\n\n\nWith the database credentials defined and the keystore generated, inject them into the cluster as Kubernetes Secrets.\nDatabase Credentials Secret:\nThe ODM application requires a Kubernetes Secret to authenticate with the database. Choose the option matching your deployment strategy.\n\nOption A: Oracle (External)Option B: Lab (Internal DB)Option C: Lab (Postgres)\n\n\nTarget: Pilot environment using external Oracle.\nCreate a secret containing the credentials for your external Oracle instance.\nkubectl create secret generic odm-db-secret \\\n  --namespace odm-pilot \\\n  --from-literal=db-user='ODM_USER' \\\n  --from-literal=db-password='StrongPassword123' \\\n  --from-literal=db-name=freepdb1 \\\n  --from-literal=db-server=oracle.cxxxxx.us-east-1.rds.amazonaws.com\n# for lab Oracle use `oracle-db.oracle.svc.cluster.local` as db-server\nNote: Ensure the secret name (odm-db-secret) matches the secretCredentials field in your values-prod.yaml.\n\n\nTarget: Sandbox / Local Lab using the internal containerized database.\nCreate a simple secret for the internal PostgreSQL container.\nkubectl create secret generic odm-db-secret \\\n  --namespace odm-pilot \\\n  --from-literal=db-user=odm \\\n  --from-literal=db-password='StrongPassword123!'\nNote: Ensure the secret name (odm-db-secret) matches the secretCredentials field in your values-lab.yaml.\n\n\nTarget: Sandbox / Local Lab using the “external” postgres database.\nCreate a simple secret for the PostgreSQL database running in the postgres namespace.\nkubectl create secret generic odm-db-secret \\\n  --namespace odm-pilot \\\n  --from-literal=db-user='odm' \\\n  --from-literal=db-password='StrongPassword123!' \\\n  --from-literal=db-server='postgres.postgres.svc.cluster.local'\nNote: Ensure the secret name (odm-db-secret) matches the secretCredentials field in your values-lab.yaml.\n\n\n\nTLS Keystore Secret: This will be referenced by the Ingress resource (tlsSecretRef) to enable HTTPS.\n# Create a standard Kubernetes TLS secret type\nkubectl create secret tls odm-tls-secret \\\n  --namespace odm-pilot \\\n  --key odm-lab.key \\\n  --cert odm-lab.crt\n\n\n\n\n\n\nNoteSecret Management\n\n\n\nIn a production environment, avoid creating secrets from literals in the CLI history. Use an External Secrets Operator (ESO) to sync these values from AWS Secrets Manager or HashiCorp Vault.\n\n\n\n\n\nKubernetes requires authentication credentials to pull container images. Depending on your environment constraints (Lab vs. Restricted Production), the source registry and credentials will differ.\n\nOption A: Private Registry (Production/Restricted)Option B: IBM Registry (Standard Lab)Option C: Lab Jenkins Deployment (PostgreSQL)\n\n\nTarget Environment: Customer Pilot / Air-Gapped / OPA-Enforced\nIn strict environments where public internet access is blocked or OPA forbids public registries, you must pull from the internal location where you mirrored the images (e.g., Artifactory).\nAction: Create a secret using your internal registry credentials.\n# Replace with your internal registry details\nkubectl create secret docker-registry internal-registry-secret \\\n  --docker-server=artifactory.internal.corp:8443 \\\n  --docker-username=&lt;SERVICE_ACCOUNT_USER&gt; \\\n  --docker-password=&lt;SERVICE_ACCOUNT_TOKEN&gt; \\\n  --docker-email=admin@internal.corp \\\n  -n odm-pilot\n\n\nTarget Environment: Sandbox / POC with Internet Access\nIf you are working in a lab with direct internet access and no strict OPA registry constraints, you can pull directly from IBM.\nAction: Create a secret using your IBM Entitlement Key.\n# 1. Get your key from myibm.ibm.com/products-services/containerlibrary\n# 2. Create the secret\nkubectl create secret docker-registry internal-registry-secret \\\n  --docker-server=cp.icr.io \\\n  --docker-username=cp \\\n  --docker-password=&lt;YOUR_IBM_ENTITLEMENT_KEY&gt; \\\n  --docker-email=user@example.com \\\n  -n odm-pilot\n\n\nInside your Jenkins Deployment, navigate to:\nManage Jenkins → Credentials → Global → Add Credentials\nFill in the fields below:\n\n\n\nField\nValue\n\n\n\n\nKind\nSecret Text\n\n\nScope\nGlobal\n\n\nSecret\nYOUR_IBM_ENTITLEMENT_KEY\n\n\nID\nicr-entitlement-key\n\n\n\nClick Create.\nAdd the following stage after the Deploy Postgres (Lab) stage\nstage('Create ICR Pull Secret') {\n  steps {\n    script {\n      withCredentials([\n        string(credentialsId: 'icr-entitlement-key', variable: 'ICR_KEY')\n      ]) {\n        withKubeConfig(credentialsId: 'k3s-kubeconfig') {\n          sh '''\necho \"Creating docker-registry secret for cp.icr.io...\"\n\nNAMESPACE=\"default\"\nSECRET_NAME=\"icr-secret\"\n\n${KUBECTL} -n ${NAMESPACE} create secret docker-registry ${SECRET_NAME} \\\n  --docker-server=cp.icr.io \\\n  --docker-username=cp \\\n  --docker-password=${ICR_KEY} \\\n  --docker-email=dummy@example.com \\\n  --dry-run=client -o yaml | ${KUBECTL} apply -f -\n'''\n        }\n      }\n    }\n  }\n}\n\n\n\n\n\n\n\n\n\nImportantSecret Name Consistency\n\n\n\nWhichever option you choose, ensure the secret name used in the kubectl create command exactly matches the value in your values.yaml file under image.pullSecrets.",
    "crumbs": [
      "Implementation Methodology",
      "Foundation & Prereqs"
    ]
  },
  {
    "objectID": "src/landing_page/landing_page.html",
    "href": "src/landing_page/landing_page.html",
    "title": "Project Name",
    "section": "",
    "text": "Project Name\nSubtitle\n\n\nOur Documentation \n\n\n\n\n\nNext Steps\n\n\n\nL ink 1\n\n\n\nLink 2"
  },
  {
    "objectID": "src/solution_overview/environment.html",
    "href": "src/solution_overview/environment.html",
    "title": "Environment Architecture",
    "section": "",
    "text": "This section details the reference architecture for the IBM ODM v9.5 deployment. The environment is designed to operate within a “Restricted” Kubernetes security context, utilizing externalized persistence and strict traffic controls to satisfy organizational compliance standards.",
    "crumbs": [
      "Solution Overview",
      "Environment"
    ]
  },
  {
    "objectID": "src/solution_overview/environment.html#logical-topology",
    "href": "src/solution_overview/environment.html#logical-topology",
    "title": "Environment Architecture",
    "section": "Logical Topology",
    "text": "Logical Topology\nThe deployment architecture isolates the ODM application logic from the persistence layer, ensuring that all compute resources within the Kubernetes cluster remain stateless and ephemeral. The solution utilizes a single-namespace model where application pods connect securely to externalized infrastructure services.\n\nCompute: Stateless Pods running on AWS EKS Worker Nodes.\nPersistence: External Oracle for transactional data.\nArtifacts: Internal Trusted Registry (Artifactory) for container images.\nRouting: Ingress Controller handling TLS termination and routing to internal ClusterIP services.\n\n\n\n\n\n\ngraph LR\n    %% ---------------------------------------------------------\n    %% STYLING DEFINITIONS\n    %% ---------------------------------------------------------\n    classDef darkBlue fill:#002C6D,stroke:#333,stroke-width:2px,color:white;\n    classDef lightBlue fill:#6BA2C1,stroke:#333,stroke-width:2px,color:white;\n    classDef green fill:#368727,stroke:#333,stroke-width:2px,color:white;\n    classDef white fill:#ffffff,stroke:#333,stroke-width:1px,color:black;\n    %% Invisible style for spacing wrappers\n    classDef invisible fill:none,stroke:none,color:none;\n\n    %% ---------------------------------------------------------\n    %% NODE DEFINITIONS\n    %% ---------------------------------------------------------\n    subgraph CN [\"Corporate Network\"]\n        User((\"User\"))\n        Admin((\"Admin\"))\n    end\n\n    subgraph AWS_CLOUD [\"AWS Cloud Environment\"]\n        %% SPACER 1: Pushes content away from 'AWS Cloud Environment' title\n        subgraph CLOUD_SPACER [\" \"]\n            direction LR\n            \n            ALB[\"AWS ALB&lt;br/&gt;(Terminates & Re-encrypts)\"]\n\n            subgraph EKS [\"AWS EKS Cluster\"]\n                %% SPACER 2: Pushes content away from 'AWS EKS Cluster' title\n                subgraph EKS_SPACER [\" \"]\n                    direction LR\n                    \n                    subgraph NS [\"Namespace: odm-pilot\"]\n                        %% SPACER 3: Pushes content away from 'Namespace' title\n                        subgraph NS_SPACER [\" \"]\n                            \n                            subgraph ODM [\"ODM Workload (UID 1001)\"]\n                                DC[\"Decision Center\"]\n                                DR[\"Decision Runner\"]\n                                DSC[\"DS Console\"]\n                                DSR[\"DS Runtime\"]\n                            end\n                        end\n                    end\n                end\n            end\n\n            Database[(\"Oracle\")]\n        end\n    end\n\n    subgraph EXT [\"External Infrastructure\"]\n        Registry[(\"Internal Artifactory\")]\n    end\n\n    %% ---------------------------------------------------------\n    %% CONNECTIONS\n    %% ---------------------------------------------------------\n    \n    %% Inbound HTTPS\n    User --&gt;|\"HTTPS\"| ALB\n    Admin --&gt;|\"HTTPS\"| ALB\n    \n    %% Internal Re-encrypted HTTPS\n    ALB --&gt;|\"HTTPS\"| DC\n    ALB --&gt;|\"HTTPS\"| DR\n    ALB --&gt;|\"HTTPS\"| DSC\n    ALB --&gt;|\"HTTPS\"| DSR\n\n    %% Database Connectivity\n    DC --&gt;|\"JDBC/TCP 1521\"| Database\n    DR --&gt;|\"JDBC/TCP 1521\"| Database\n    DSC --&gt;|\"JDBC/TCP 1521\"| Database\n    DSR --&gt;|\"JDBC/TCP 1521\"| Database\n\n    %% Image Pulls\n    DC -.-&gt;|\"Image Pull\"| Registry\n    DR -.-&gt;|\"Image Pull\"| Registry\n    DSC -.-&gt;|\"Image Pull\"| Registry\n    DSR -.-&gt;|\"Image Pull\"| Registry\n\n    %% ---------------------------------------------------------\n    %% APPLY STYLES\n    %% ---------------------------------------------------------\n    class User,Admin darkBlue;\n    class DC,DR,DSC,DSR lightBlue;\n    class Database,ALB green;\n    class Registry white;\n    \n    %% Apply invisible style to all spacer subgraphs\n    class CLOUD_SPACER,EKS_SPACER,NS_SPACER invisible;",
    "crumbs": [
      "Solution Overview",
      "Environment"
    ]
  },
  {
    "objectID": "src/solution_overview/environment.html#component-matrix",
    "href": "src/solution_overview/environment.html#component-matrix",
    "title": "Environment Architecture",
    "section": "Component Matrix",
    "text": "Component Matrix\nThe solution validates the integration of the following specific software versions.\n\n\n\n\n\n\n\n\nComponent\nVersion\nRole\n\n\n\n\nPlatform\nAWS EKS (K8s 1.24+)\nContainer Orchestration Platform\n\n\nSoftware\nIBM ODM 9.5.0.1\nBusiness Rule Management System\n\n\nHelm Chart\nibm-odm-prod 25.1.0\nDeployment Manager\n\n\nDatabase\nOracle 19c or 23ai\nExternal Persistence Layer (AWS RDS or EC2)\n\n\nPolicy Engine\nOPA Gatekeeper\nSecurity Governance & Admission Control\n\n\nIngress\nNGINX / AWS ALB\nTraffic Routing & TLS Termination",
    "crumbs": [
      "Solution Overview",
      "Environment"
    ]
  },
  {
    "objectID": "src/solution_overview/environment.html#security-context-specification",
    "href": "src/solution_overview/environment.html#security-context-specification",
    "title": "Environment Architecture",
    "section": "Security Context Specification",
    "text": "Security Context Specification\nTo comply with the Restricted Pod Security Standards enforced by OPA Gatekeeper, the ODM application containers are configured with a strict security profile. This profile overrides standard defaults to ensure “Zero Privilege” execution.\n\nPod Security Settings\nThe deployment pipeline explicitly injects the following contexts into all workload resources:\n\nUser ID (UID): 1001 (Non-Root)\nGroup ID (GID): 1001 (Non-Root)\nFilesystem: Read-Only Root Filesystem (with specific volume mounts for temp directories)\nPrivilege Escalation: AllowPrivilegeEscalation: false\nCapabilities: DROP ALL\nSeccomp Profile: RuntimeDefault\n\n\n\n\n\n\n\nNote\n\n\n\nService Account Token: To minimize the attack surface, automountServiceAccountToken is disabled on application pods. This configuration is validated for core ODM functionality, though it restricts the usage of the standard IBM License Metering agent sidecar.",
    "crumbs": [
      "Solution Overview",
      "Environment"
    ]
  },
  {
    "objectID": "src/solution_overview/environment.html#network-connectivity",
    "href": "src/solution_overview/environment.html#network-connectivity",
    "title": "Environment Architecture",
    "section": "Network & Connectivity",
    "text": "Network & Connectivity\nThe environment assumes a “Deny by Default” network posture.\n\nIngress (Inbound)\n\nProtocol: HTTPS Only (HTTP traffic is strictly disabled at the Ingress level).\nTermination: TLS is terminated at the Ingress Controller using a Kubernetes Secret.\nRouting: Traffic is routed to internal ClusterIP services. No NodePorts or LoadBalancers are created directly by the application.\n\n\n\nEgress (Outbound)\nThe ODM Pods require outbound network access to the following destinations:\n\nDatabase: TCP access to the AWS RDS endpoint (typically port 5432).\nImage Registry: HTTPS access to the internal Artifactory for image pulling.\nInternal DNS: UDP/TCP access to the cluster CoreDNS service.",
    "crumbs": [
      "Solution Overview",
      "Environment"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Secure Deployment of IBM ODM on AWS EKS",
    "section": "",
    "text": "MD",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "index.html#description-secure-deployment-of-ibm-odm-on-aws-eks",
    "href": "index.html#description-secure-deployment-of-ibm-odm-on-aws-eks",
    "title": "Secure Deployment of IBM ODM on AWS EKS",
    "section": "Description: “Secure Deployment of IBM ODM on AWS EKS”",
    "text": "Description: “Secure Deployment of IBM ODM on AWS EKS”",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "index.html#the-why",
    "href": "index.html#the-why",
    "title": "Secure Deployment of IBM ODM on AWS EKS",
    "section": "The Why",
    "text": "The Why\nThe client is executing a strategic initiative to modernize their business rule management systems, transitioning critical decision logic from the mainframe to a cloud-native architecture on AWS EKS. This modernization effort requires the agility of containerization without compromising the rigorous security standards inherent to the financial services sector.\n\nBalancing Agility and Security: The primary challenge lies in reconciling the rapid deployment capabilities of IBM ODM with the client’s “Zero Exception” security posture.\nStrict Compliance alignment: Financial services environments often enforce “Restricted” Pod Security Standards that exceed standard Kubernetes defaults. Deploying complex enterprise software into these environments requires bridging the gap between standard deployment manifests and bespoke security constraints.\nOperational Standardization: The client requires a repeatable, automated deployment pipeline that satisfies automated policy gates (OPA Gatekeeper) without requiring manual intervention or policy waivers.\n\n\nProblem Details\nTechnical Hurdles & Policy Integration\nThe objective is to deploy IBM Operational Decision Manager (ODM) v9.5 into an AWS EKS cluster protected by OPA Gatekeeper constraints. While the IBM ODM Helm Chart covers the vast majority of Kubernetes security configurations, this specific environment enforces a distinct set of “Restricted” policies that require granular control over every aspect of the Pod Security Context.\nThe specific integration challenges addressed in this solution include:\n\nGranular Security Context Enforcement: The target environment mandates explicit definitions for runAsGroup and supplementalGroups at the Pod level. The solution requires a method to inject these specific, client-mandated security contexts into the standard deployment manifests during the CI/CD process.\nZero-Trust Persistence Architecture: The standard internal database configuration utilizes specific filesystem groups (fsGroup) for storage permission management. To align with the client’s non-root requirements (UID/GID &gt; 1000), the architecture must be adapted to utilize an External Database (e.g. AWS RDS) rather than containerized persistence.\nSecure Supply Chain: The environment prohibits pulling images from public or vendor registries. The deployment workflow must accommodate an air-gapped supply chain, utilizing an internal artifact repository and referencing all container images strictly by SHA256 digests.\nIngress Hardening: To meet strict traffic control policies, the solution must explicitly disable HTTP pathways and enforce TLS termination and specific ingress host matching at the manifest level.\n\n\n\nAdditional Context\nEnvironment & Constraints\n\nPlatform: AWS Elastic Kubernetes Service (EKS).\nPolicy Engine: Open Policy Agent (OPA) Gatekeeper enforcing the Kubernetes “Restricted” Pod Security Standard.\nConstraint Strictness:\n\nImmutable Policies: The platform team does not allow policy exceptions or namespace-level whitelisting.\nPre-Deployment Validation: Deployment manifests must be fully compliant before applying to the cluster; relying on post-admission mutations is not permitted.\n\nSoftware Version: IBM ODM 9.5.0.1 (Helm Chart 25.1.0).\nNetwork Posture: Strict egress filtering requires explicit allowance for connectivity to external services (RDS) and internal tooling (Artifactory).",
    "crumbs": [
      "Problem Definition"
    ]
  },
  {
    "objectID": "src/solution_overview/prepare.html",
    "href": "src/solution_overview/prepare.html",
    "title": "Deployment Readiness",
    "section": "",
    "text": "Successful deployment of IBM ODM into a strictly governed EKS environment requires specific infrastructure pillars to be established prior to installation. This section outlines the necessary tooling, external services, and cluster configurations required to satisfy the Restricted OPA Gatekeeper policies.",
    "crumbs": [
      "Solution Overview",
      "Prepare"
    ]
  },
  {
    "objectID": "src/solution_overview/prepare.html#prerequisite-checklist",
    "href": "src/solution_overview/prepare.html#prerequisite-checklist",
    "title": "Deployment Readiness",
    "section": "Prerequisite Checklist",
    "text": "Prerequisite Checklist\nBefore initiating the deployment pipeline, ensure the following prerequisites are met.\n\n1. Workstation & Tooling\nThe automation bastion or engineer’s workstation requires connectivity to the target EKS cluster and the following CLI tools:\n\nHelm 3 (v3.10+ recommended) for chart management.\nKubectl configured with the correct context.\nKustomize (v4+ or built-in via kubectl -k) for manifest post-rendering.\nOpenShift CLI (oc) or Skopeo (Optional): Recommended tools for manual image mirroring if an automated enterprise pipeline is not available.\n\n\n\n\n\n\n\nTipHelm 3 install on Linux (RHEL 8):\n\n\n\ncurl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash\n\n\n\n\n2. Database Strategy\nThe standard containerized database included with the product requires privileged filesystem groups (fsGroup: 26), which is incompatible with the target environment’s OPA policies.\nPlease select ONE of the following compliant strategies for this deployment:\nOption A: External Oracle Database (Production Recommended) Utilize a managed database service or self-managed EC2 Oracle database to completely offload persistence management and security compliance from the Kubernetes cluster.\n\nRequirement: Oracle Database 19c or 23ai.\n\nConfiguration: Ensure the database is accessible from the EKS Node Security Group (port 1521).\n\nCredentials: Obtain the Endpoint URL, Port, Oracle Service Name (preferred over SID), Username, and Password.\n\nOption B: Compliant Internal Container (Pilot/PoC Alternative) For non-production pilot environments, the you may provide their own approved PostgreSQL container image to run within the same namespace.\n\nRequirement: The container image must be security-hardened and configured to run as a non-root user (UID &gt; 1001) to pass OPA checks.\n\nConfiguration: The client is responsible for defining the deployment and storage resources for this custom database container.\n\n\n\n3. Supply Chain (Image Mirroring)\nThe target environment prohibits direct access to public registries (cp.icr.io). All container images must be staged in the client’s internal trusted registry (e.g., Artifactory).\n\n\n\n\n\n\nNoteIBM entitlement key\n\n\n\nYou will need an IBM entitlement key to mirror the images or install without mirroring. This can be obtained here.\n\n\nEnterprise Pipeline Integration If the organization utilizes a centralized image ingestion process, please configure the pipeline to pull from the IBM Entitled Registry using the source details below.\n\nSource Registry: cp.icr.io\nSource Namespace: cp/cp4a/odm\nTag: 9.5.0.1\n\nRequired Images:\n\n\n\nComponent\nImage Name\n\n\n\n\nDecision Center\nodm-decisioncenter\n\n\nDecision Runner\nodm-decisionrunner\n\n\nDecision Server Console\nodm-decisionserverconsole\n\n\nDecision Server Runtime\nodm-decisionserverruntime\n\n\nDatabase Init Utility\ndbserver\n\n\n\n\n\n\n\n\n\nTipManual Mirroring Alternative\n\n\n\nIn the absence of an automated ingestion pipeline, we recommend using the OpenShift CLI (oc) or Skopeo. These tools efficiently copy multi-architecture manifest lists between registries without requiring intermediate disk storage or Docker daemons.\n\n\n\n\n4. Cluster Configuration\nThe Kubernetes namespace must be prepared with the necessary secrets and networking definitions.\n\nNamespace: Create a dedicated namespace (e.g., odm-pilot).\nTLS Secret: A pre-provisioned Kubernetes Secret (type kubernetes.io/tls) containing the valid certificate and private key for the Ingress controller.\n\nRequirement: The certificate Subject Alternative Name (SAN) must match the intended Ingress host (e.g., odm.internal.corp).\nNote: In a production environment, this secret is typically provisioned by the organization’s PKI automation (e.g., Cert-Manager or Venafi). For Lab/Pilot implementation steps, see the Implementation Methodology section.\n\nStorage Class (Conditional):\n\nIf using Option B (Internal Container): A Storage Class must be identified to provision the Persistent Volume for the database.\nIf using Option A (AWS RDS): No Storage Class is required for the database layer.\n\n\n\n\n\n\n\n\nImportantOPA Policy Alignment\n\n\n\nEnsure that the Namespace does not have any legacy PodSecurityPolicies attached that might conflict with the OPA Gatekeeper constraints. The solution relies entirely on the OPA constraints for security governance.\n\n\n\n\n\n\n\n\nTipLab & Sandbox Setup\n\n\n\nIf you are attempting to reproduce this environment in a local lab (e.g., k3s or Minikube) and need to manually install OPA Gatekeeper to simulate the constraint layer, please refer to the Lab Setup Guide: OPA Gatekeeper Configuration.",
    "crumbs": [
      "Solution Overview",
      "Prepare"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html",
    "href": "src/solution_overview/troubleshooting.html",
    "title": "Troubleshooting & Diagnostics",
    "section": "",
    "text": "This section outlines common issues encountered during the deployment of Operational Decision Manager (ODM) on Amazon EKS. It focuses specifically on the challenges introduced by the strict security requirements: end-to-end TLS encryption (HTTPS everywhere) and restrictive database network policies.",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#overview",
    "href": "src/solution_overview/troubleshooting.html#overview",
    "title": "Troubleshooting & Diagnostics",
    "section": "",
    "text": "This section outlines common issues encountered during the deployment of Operational Decision Manager (ODM) on Amazon EKS. It focuses specifically on the challenges introduced by the strict security requirements: end-to-end TLS encryption (HTTPS everywhere) and restrictive database network policies.",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#logging-strategy",
    "href": "src/solution_overview/troubleshooting.html#logging-strategy",
    "title": "Troubleshooting & Diagnostics",
    "section": "Logging Strategy",
    "text": "Logging Strategy\nEffective troubleshooting requires inspecting logs at three distinct layers. Use the following commands to isolate errors.\n\nODM Application Logs\nThe most critical logs are generated by the WebSphere Liberty server running inside the ODM pods. Look here for JDBC connection errors, rule execution failures, or startup timeouts.\n# Stream logs for the Decision Center pod\nkubectl logs -f -l app=odm-decision-center -n odm-pilot\n\n# Check for specific \"messages.log\" errors inside a running container\nkubectl exec -it &lt;pod-name&gt; -n odm-pilot -- cat /logs/messages.log\n\n\nAWS Load Balancer Controller Logs\nIf the Ingress resource is created but the AWS ALB or Target Groups are not provisioning, inspect the controller logs in the kube-system namespace.\nkubectl logs -f -n kube-system -l app.kubernetes.io/name=aws-load-balancer-controller\n\n\nKubernetes Events\nUse events to diagnose scheduling issues, resource quota limits, or image pull failures.\nkubectl get events -n odm-pilot --sort-by='.lastTimestamp'",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#common-failure-scenarios",
    "href": "src/solution_overview/troubleshooting.html#common-failure-scenarios",
    "title": "Troubleshooting & Diagnostics",
    "section": "Common Failure Scenarios",
    "text": "Common Failure Scenarios\nUse this matrix to quickly resolve the most frequent deployment errors.\n\n\n\n\n\n\n\n\nSymptom\nProbable Cause\nCorrective Action\n\n\n\n\nALB returns 502 Bad Gateway\nProtocol Mismatch. The ALB is sending HTTP to a Pod expecting HTTPS, or the Pod is failing its readiness probe.\n1. Verify the Ingress annotation exists: alb.ingress.kubernetes.io/backend-protocol: HTTPS2. Check if the Pod’s Readiness Probe is failing (see Section 1.3).\n\n\nPod stuck in CrashLoopBackOff\nDatabase Failure. The ODM container cannot reach RDS to initialize the schema.\n1. Check pod logs for Connection refused or SocketTimeout.2. Verify the RDS Security Group allows inbound traffic on port 5432 from the EKS Node Security Group.\n\n\nDeployment fails OPA Validation\nPolicy Violation. The deployment spec exposes a non-secure port (HTTP).\nEnsure the container spec creates the service on port 9443 (HTTPS) rather than 9060. The OPA policy mandates secure listeners.\n\n\n“Datasource not found” Error\nConfig Error. The PostgreSQL driver isn’t loaded, or the datasource XML is malformed.\nVerify that the PostgreSQL JDBC driver is mounted correctly in the container’s shared/resources volume and referenced in server.xml.\n\n\nALB Target Group is “Unhealthy”\nHealth Check Mismatch. The ALB is health-checking the wrong port or protocol.\nVerify the Ingress health check annotations. Ensure alb.ingress.kubernetes.io/healthcheck-protocol is set to HTTPS and points to a valid endpoint (e.g., /decisioncenter/health).",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#deep-dive-tls-ingress",
    "href": "src/solution_overview/troubleshooting.html#deep-dive-tls-ingress",
    "title": "Troubleshooting & Diagnostics",
    "section": "Deep Dive: TLS & Ingress",
    "text": "Deep Dive: TLS & Ingress\nBecause of the strict OPA requirement for HTTPS traffic at the pod level, the connection between the AWS ALB and the Pods is the most fragile configuration point.\n\nInfinite Redirect Loops\nContext: ODM often attempts to redirect traffic internally, which can conflict with ALB redirects. Fix: Ensure the annotation alb.ingress.kubernetes.io/ssl-redirect: '443' is configured. Additionally, verify the ODM Liberty server configuration (server.xml) is configured to trust the proxy headers (X-Forwarded-Proto) coming from the ALB so it recognizes the original request was secure.\n\n\nUntrusted Certificates\nContext: The ALB attempts to connect to the Pod via HTTPS, but the Pod is using a self-signed certificate. Fix: By default, the AWS ALB accepts self-signed certificates from backends when the backend protocol is HTTPS. However, you must ensure the Pod is actually listening on port 9443.\n\n\n\n\n\n\nTipTesting Internal Connectivity\n\n\n\nYou can test the internal TLS handshake from within the cluster using a temporary debug pod.\n# Run a temporary curl pod\nkubectl run curl-debug --image=curlimages/curl -n odm-pilot --rm -it -- sh\n\n# Inside the pod, test the handshake (-k allows self-signed certs)\ncurl -k -v https://&lt;odm-pod-ip&gt;:9443/decisioncenter",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#database-connectivity-checklist",
    "href": "src/solution_overview/troubleshooting.html#database-connectivity-checklist",
    "title": "Troubleshooting & Diagnostics",
    "section": "Database Connectivity Checklist",
    "text": "Database Connectivity Checklist\nIf the Decision Center or Decision Server Console fails to start, 90% of the time it is a Database connectivity or permission issue.\n\n\n\n\n\n\nImportantCritical Checks\n\n\n\n\nVPC Peering/Routing: Ensure the EKS VPC has a valid route table entry to the RDS subnet.\nSecurity Groups:\n\nSource: The Security Group attached to the EKS Worker Nodes.\nDestination: The Security Group attached to RDS (Must allow TCP/5432).\n\nSchema Privileges: If this is a fresh install, ensure the user provided in the JDBC connection string has CREATE TABLE privileges. ODM must create its own schema tables on the very first startup.",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/solution_overview/troubleshooting.html#opa-gatekeeper-diagnostics",
    "href": "src/solution_overview/troubleshooting.html#opa-gatekeeper-diagnostics",
    "title": "Troubleshooting & Diagnostics",
    "section": "OPA Gatekeeper Diagnostics",
    "text": "OPA Gatekeeper Diagnostics\nSince the cluster enforces strict security policies (such as requiring HTTPS listeners), OPA Gatekeeper acts as the admission controller. If your deployments fail validation, use the following commands to investigate why.\n\nIdentifying Policy Violations\nIf a kubectl apply or Helm install fails with an error message like Error from server (Forbidden): admission webhook \"validation.gatekeeper.sh\" denied the request, follow these steps to identify the blocking policy.\n1. List Active Constraints View all constraints currently enforced on the cluster to find the relevant policy (e.g., k8srequiredlabels, k8shttpsonly, etc.).\nkubectl get constraints\n2. Inspect Specific Violations If a resource is blocked or audited, the details are stored in the status field of the Constraint object. This will tell you exactly which field in your manifest failed validation.\n# Syntax: kubectl describe &lt;ConstraintKind&gt; &lt;ConstraintName&gt;\n# Example: Checking for ingress security violations\nkubectl describe k8shttpsonly ingress-must-be-secure\nLook for the Total Violations field and the Violations list in the output.\n\n\nDebugging Policy Logic\nIf you suspect a policy is misconfigured or behaving unexpectedly (e.g., blocking valid resources), you can inspect the underlying Rego logic or check the Gatekeeper controller logs.\nInspect the Constraint Template (Rego) The logic resides in the ConstraintTemplate. Inspecting this allows you to see the actual Rego code being executed.\n# List available templates\nkubectl get constrainttemplates\n\n# View the Rego logic for a specific template\nkubectl get constrainttemplate k8shttpsonly -o yaml\nCheck Gatekeeper Controller Logs The controller logs provide detailed information on webhook admission requests, including the JSON payload that was sent to OPA.\nkubectl logs -l control-plane=controller-manager -n gatekeeper-system\n\n\n\n\n\n\nNoteDry Run Mode\n\n\n\nIf you are debugging a new policy and want to observe violations without blocking deployments, you can temporarily set the enforcement action to dryrun in the Constraint YAML:\nspec:\n  enforcementAction: dryrun\nViolations will appear in the Constraint status (via kubectl describe) but will not block the creation or update of resources.\n\n\n\n\nView All Violations Cluster-Wide\nTo get a comprehensive list of every active policy violation in the cluster (audited vs. blocked), you can use this formatted command. It iterates through every Constraint and prints the resource name and specific error message for each violation.\nkubectl get constraints -o jsonpath='{range .items[*]}{\"POLICY: \"}{.metadata.name}{\"\\n\"}{range .status.violations[*]}{\"  FAIL: \"}{.message}{\"\\n    -&gt; Resource:  \"}{.kind}/{.name}{\"\\n\"}{end}{\"\\n\"}{end}'\nIf you only want violations from the odm-pilot namespace:\nkubectl get constraints -o jsonpath='{range .items[*]}{\"POLICY: \"}{.metadata.name}{\"\\n\"}{range .status.violations[?(@.namespace==\"odm-pilot\")]}{\"  FAIL: \"}{.message}{\"\\n    -&gt; Resource:  \"}{.kind}/{.name}{\"\\n\"}{end}{\"\\n\"}{end}'\nOutput Example:\nPOLICY: ingress-must-be-secure\n  FAIL: Ingress must be https only\n    -&gt; Resource:  Ingress/odm-ingress\n\nPOLICY: container-must-have-probes\n  FAIL: Container &lt;decision-center&gt; is missing a readinessProbe\n    -&gt; Resource:  Pod/odm-decision-center-0",
    "crumbs": [
      "Solution Overview",
      "Troubleshooting"
    ]
  },
  {
    "objectID": "src/key-takeaway.html",
    "href": "src/key-takeaway.html",
    "title": "Key Takeaways",
    "section": "",
    "text": "Document Status: Pre-Pilot\nThis section is currently a placeholder. Following the execution of the pilot phase, this section will be updated to reflect the actual outcomes, technical hurdles overcome, and validated best practices. The structure below outlines the key metrics and qualitative data points to be captured.\n\n\n\n\n\n\nSecurity Validation: [Pending] Confirm if OPA Gatekeeper successfully blocked non-compliant resources without impeding valid deployments.\nConnectivity: [Pending] Verify if the End-to-End TLS chain (User -&gt; ALB -&gt; Pod) functioned as expected with the self-signed internal certificates.\nPerformance: [Pending] Observations regarding the startup time of ODM pods when connecting to Oracle compared to local/on-prem databases.\n\n\n\n\n\nConfiguration Challenges: [Insert details on any specific Helm chart overrides or Ingress annotations that required significant debugging].\nOperational Friction: [Note any difficulties the team faced when managing secrets or certificates manually].\nUnexpected Behavior: [Document any AWS-specific quirks encountered, such as ALB provisioning delays or Security Group mapping issues].\n\n\n\n\n\nAutomation: [Plan to transition from manual secret creation to External Secrets Operator?].\nObservability: [Plan for integrating CloudWatch or Prometheus to monitor the decision runner performance?].\nScaling: [Recommendations for HPA (Horizontal Pod Autoscaler) based on the pilot load testing?].",
    "crumbs": [
      "Key Takeaways"
    ]
  },
  {
    "objectID": "src/key-takeaway.html#key-takeaways",
    "href": "src/key-takeaway.html#key-takeaways",
    "title": "Key Takeaways",
    "section": "",
    "text": "Document Status: Pre-Pilot\nThis section is currently a placeholder. Following the execution of the pilot phase, this section will be updated to reflect the actual outcomes, technical hurdles overcome, and validated best practices. The structure below outlines the key metrics and qualitative data points to be captured.\n\n\n\n\n\n\nSecurity Validation: [Pending] Confirm if OPA Gatekeeper successfully blocked non-compliant resources without impeding valid deployments.\nConnectivity: [Pending] Verify if the End-to-End TLS chain (User -&gt; ALB -&gt; Pod) functioned as expected with the self-signed internal certificates.\nPerformance: [Pending] Observations regarding the startup time of ODM pods when connecting to Oracle compared to local/on-prem databases.\n\n\n\n\n\nConfiguration Challenges: [Insert details on any specific Helm chart overrides or Ingress annotations that required significant debugging].\nOperational Friction: [Note any difficulties the team faced when managing secrets or certificates manually].\nUnexpected Behavior: [Document any AWS-specific quirks encountered, such as ALB provisioning delays or Security Group mapping issues].\n\n\n\n\n\nAutomation: [Plan to transition from manual secret creation to External Secrets Operator?].\nObservability: [Plan for integrating CloudWatch or Prometheus to monitor the decision runner performance?].\nScaling: [Recommendations for HPA (Horizontal Pod Autoscaler) based on the pilot load testing?].",
    "crumbs": [
      "Key Takeaways"
    ]
  },
  {
    "objectID": "src/implementation_methodology/stepthree-imp.html",
    "href": "src/implementation_methodology/stepthree-imp.html",
    "title": "Step Three",
    "section": "",
    "text": "Step Three Implementation\nPhasellus at risus egestas, ultricies tortor efficitur, auctor augue. Suspendisse finibus maximus dui nec condimentum. Proin fringilla efficitur vehicula. Suspendisse sem lacus, iaculis quis erat et, facilisis sagittis est. Sed sapien justo, condimentum vitae aliquet sed, faucibus at ex. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Fusce placerat ante eget diam tincidunt, vitae tincidunt sapien malesuada. Nullam ullamcorper justo eros. Duis ultricies aliquam dui, id aliquam lorem congue vitae. Ut porttitor, tellus eu dignissim semper, turpis nunc cursus libero, sed placerat est dui non enim.\nVestibulum libero dolor, vehicula vitae risus non, consequat gravida neque. Maecenas a posuere sem. Quisque dignissim porta pretium. Nam mattis lacus commodo lobortis consequat. Vestibulum at massa a quam egestas accumsan. Fusce ac neque eu libero maximus aliquet id quis metus. In sodales neque ut turpis iaculis vehicula. Sed volutpat, tellus non laoreet aliquet, risus felis ornare dolor, interdum venenatis turpis metus in lorem. Aliquam quam ligula, porttitor non ultrices ac, lacinia ullamcorper massa. Integer molestie eleifend urna, imperdiet dignissim tellus malesuada ac. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam tristique sem ac purus interdum, vitae pharetra erat fringilla. Integer non nunc a eros porttitor rutrum. Suspendisse ut libero urna. Vestibulum vitae est at diam vestibulum aliquet. Donec porta nunc eget lobortis mollis.\nVestibulum luctus sodales odio, at luctus lacus lobortis a. Aliquam vulputate id turpis sit amet bibendum. Donec a dignissim tellus, vel vehicula erat. Pellentesque hendrerit ex magna, at dictum est pretium a. Vivamus faucibus ipsum lectus, ut elementum diam interdum scelerisque. Cras magna sapien, tincidunt quis eleifend at, tincidunt non eros. Aliquam ac augue turpis."
  }
]