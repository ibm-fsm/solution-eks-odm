---
title: "Phase 1: Foundation & Prerequisite Configuration"
format: html
date: last-modified
---

## Phase 1: Foundation & Prerequisite Configuration

Before deploying the ODM application logic, the infrastructure foundation must be secured. This phase covers provisioning the database, generating internal TLS assets for end-to-end encryption, and creating the necessary Kubernetes secrets.

### 1.0 Create namespace

Create the dedicated namespace for the ODM deployment. It is recommended to set your current context to this namespace to simplify subsequent commands.

```bash
# 1. Create the namespace
kubectl create namespace odm-pilot

# 2. Set as current context (Optional but recommended)
kubectl config set-context --current --namespace=odm-pilot
```

### 1.1 Database Provisioning (AWS RDS)
:::{.panel-tabset}

## Option A: AWS RDS
ODM requires a robust persistence layer. We utilize **AWS RDS for PostgreSQL** (v12+).

1.  **Provision RDS:** Ensure the instance is deployed in private subnets reachable by the EKS cluster.
2.  **Configure Security Groups:**
    *   **Inbound Rule:** Allow TCP/5432 from the EKS Cluster Security Group.
    *   **Outbound Rule:** Allow return traffic.


## Option B: Lab (PostgreSQL)
Create PostgreSQL in a separate name space to similate a similar setup to a prod environment with AWS RDS.

1.  **Create New Namespace (PostgreSQL)**
    From a node inside your cluster on Bastion (eg. [clouduser@my-k3s-server-0 ~])
    
    ```bash
    kubectl create namespace postgres
    ```
    **Note: **You can check namespaces with ```bash kubectl get ns```
2.  **Create PostgreSQL Secret**

    ```bash
    kubectl -n postgres create secret generic postgres-secret \
      --from-literal=POSTGRES_DB=postgres \
      --from-literal=POSTGRES_USER=postgres \
      --from-literal=POSTGRES_PASSWORD='StrongPassword123!'
    ```
3.  **Create persistent value storage**
    
    ```bash
    mkdir -p ~/k3s/postgres
    cd ~/k3s/postgres
    vi postgres-pvc.yaml
    ```

    Paste into postgres-pvc.yaml
    ```bash
    apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: postgres-pvc
      namespace: postgres
    spec:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 10Gi
    ```
    Save the file and apply it.

    ```bash 
    kubectl apply -f postgres-pvc.yaml
    ```
    Should output: ```persistentvolumeclaim/postgres-pvc created```
4.  **Create the deployment yaml**

    ```bash 
    vi postgres-deployment.yaml
    ```

    Paste the following into the yaml
    ```bash
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: postgres
      namespace: postgres
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: postgres
      template:
        metadata:
          labels:
            app: postgres
        spec:
          containers:
          - name: postgres
            image: postgres:16
            ports:
            - containerPort: 5432
            envFrom:
            - secretRef:
                name: postgres-secret
            volumeMounts:
            - name: postgres-storage
              mountPath: /var/lib/postgresql/data
          volumes:
          - name: postgres-storage
            persistentVolumeClaim:
              claimName: postgres-pvc
    ```

    Save the file and apply it.
    ```bash
    kubectl apply -f postgres-deployment.yaml
    ```

    Expected output: ```deployment.apps/postgres created```


5.  **Check that the Postgres pod is running**

    ```bash
    kubectl -n postgres get pods -w
    ```

    Expect one of the following:

    ```bash
    postgres-xxxxx   Pending
    postgres-xxxxx   ContainerCreating
    postgres-xxxxx   Running
    ```

    Eventually you should see 'Running'

6. **Check that the volume is mounted**

    ```bash
    kubectl -n postgres get pvc
    ```
    **Note:** if it gets stuck, run ```kubectl -n postgres describe pod postgres-xxxxx```
7. **Create the service file (once pod is running)**
    
    ```bash
    vi postgres-service.yaml
    ```

    Paste

    ```bash
    apiVersion: v1
    kind: Service
    metadata:
      name: postgres
      namespace: postgres
    spec:
      type: ClusterIP
      selector:
        app: postgres
      ports:
      - port: 5432
        targetPort: 5432
    ```

    Apply the yaml

    ```bash
    kubectl apply -f postgres-service.yaml
    ```

    Expected output ```service/postgres created```

8. **Verify that the volume is running**  
    
    ```bash
    kubectl -n postgres get svc
    ```

    You should now have `postgres-pvc.yaml`, `postgres-deployment.yaml`, and `postgres-service.yaml` under the `~/k3s/postgres` directory.

9. **Connect to the database to create schema and user**

    ```bash
    kubectl exec -it -n postgres \
    $(kubectl get pods -n postgres -l app=postgres --field-selector=status.phase=Running -o jsonpath="{.items[0].metadata.name}") \
    -- psql -U postgres
    ```

:::{.callout-note}
### Database Connection Details: 
The database host for the internal PostgreSQL deployment will be `postgres.postgres.svc.cluster.local` on port `5432`. Use this for configuring the ODM application connection string in the subsequent steps.
:::

:::




### 1.2 Schema & User Setup

The ODM data source requires specific privileges to initialize the schema on the first startup. Connect to your RDS instance via a bastion host or temporary pod and execute the following SQL commands:

```sql
-- 1. Create the dedicated ODM user
CREATE USER odm WITH PASSWORD 'StrongPassword123!';

-- 2. Create the database
CREATE DATABASE odm_db OWNER odm;

-- 3. Grant privileges (Required for table creation)
GRANT ALL PRIVILEGES ON DATABASE odm_db TO odm;

-- 4. (Optional) If using a specific schema
\c odm_db
CREATE SCHEMA odm_rules AUTHORIZATION odm;
```

### 1.3 Internal TLS Certificate Generation

To satisfy the OPA policy requiring HTTPS traffic at the cluster boundary, the Kubernetes Ingress resource must be configured with a valid TLS secret. This enables the Ingress Controller to terminate HTTPS traffic at the cluster boundary.

For this document, we will generate a self-signed certificate using OpenSSL.

**Generate the Certificate and Key (PEM):**

:::{.callout-note}
### Determining the Certificate Subject (CN)

The Common Name (`/CN`) in the certificate must match the exact Fully Qualified Domain Name (FQDN) that users will type into their browser.

*   **In the Lab:** We use the pattern `odm.<proxy>` (e.g., `/CN=odm.my-haproxy.gym.lan/O=Lab/C=US`). This ensures the browser accepts the certificate when traffic is routed through your lab's load balancer.
*   **In Production (AWS ALB):** Use the **Route53 CNAME** or Alias record created for the application (e.g., `/CN=odm.internal.corp`).
    *   *Important:* Do **not** use the raw AWS ALB hostname (e.g., `*.elb.amazonaws.com`) as the CN. Browser security policies will reject the certificate if it identifies the load balancer hardware rather than the application service name.
:::

```bash
# 1. Generate a self-signed certificate and private key
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout odm-lab.key \
  -out odm-lab.crt \
  -subj "/CN=odm.internal.corp/O=MyCorp/C=US"

# 2. Verify the content
ls -l odm-lab.key odm-lab.crt
```

### 1.4 Creating Kubernetes Secrets

With the database credentials defined and the keystore generated, inject them into the cluster as Kubernetes Secrets.

**Database Credentials Secret:**

The ODM application requires a Kubernetes Secret to authenticate with the database. Choose the option matching your deployment strategy.

:::{.panel-tabset}

## Option A: Production (External RDS)
**Target:** Pilot / Production environment using AWS RDS.

Create a secret containing the credentials for your external PostgreSQL instance.

```bash
kubectl create secret generic odm-db-secret \
  --namespace odm-pilot \
  --from-literal=db-user=odm \
  --from-literal=db-password='StrongPassword123!' \
  --from-literal=db-name=odm_db \
  --from-literal=db-server=postgres.cxxxxx.us-east-1.rds.amazonaws.com
```

*Note: Ensure the secret name (`odm-db-secret`) matches the `secretCredentials` field in your `values-prod.yaml`.*

## Option B: Lab (Internal DB)
**Target:** Sandbox / Local Lab using the internal containerized database.

Create a simple secret for the internal PostgreSQL container.

```bash
kubectl create secret generic odm-db-secret \
  --namespace odm-pilot \
  --from-literal=db-user=odm \
  --from-literal=db-password='StrongPassword123!'
```

*Note: Ensure the secret name (`odm-db-secret`) matches the `secretCredentials` field in your `values-lab.yaml`.*

## Option C: Lab (Postgres)
**Target:** Sandbox / Local Lab using the "external" postgres database.

Create a simple secret for the PostgreSQL database running in the `postgres` namespace.

```bash
kubectl create secret generic odm-db-secret \
  --namespace odm-pilot \
  --from-literal=db-user='odm' \
  --from-literal=db-password='StrongPassword123!' \
  --from-literal=db-server='postgres.postgres.svc.cluster.local'
```

*Note: Ensure the secret name (`odm-db-secret`) matches the `secretCredentials` field in your `values-lab.yaml`.*

:::

**TLS Keystore Secret:**
This will be referenced by the Ingress resource (`tlsSecretRef`) to enable HTTPS.

```bash
# Create a standard Kubernetes TLS secret type
kubectl create secret tls odm-tls-secret \
  --namespace odm-pilot \
  --key odm-lab.key \
  --cert odm-lab.crt
```

::: {.callout-note}
### Secret Management
In a production environment, avoid creating secrets from literals in the CLI history. Use an External Secrets Operator (ESO) to sync these values from AWS Secrets Manager or HashiCorp Vault.
:::

### 1.5 Create Image Pull Secret

Kubernetes requires authentication credentials to pull container images. Depending on your environment constraints (Lab vs. Restricted Production), the source registry and credentials will differ.

:::{.panel-tabset}

## Option A: Private Registry (Production/Restricted)
**Target Environment:** Customer Pilot / Air-Gapped / OPA-Enforced

In strict environments where public internet access is blocked or OPA forbids public registries, you must pull from the internal location where you mirrored the images (e.g., Artifactory).

**Action:** Create a secret using your internal registry credentials.

```bash
# Replace with your internal registry details
kubectl create secret docker-registry internal-registry-secret \
  --docker-server=artifactory.internal.corp:8443 \
  --docker-username=<SERVICE_ACCOUNT_USER> \
  --docker-password=<SERVICE_ACCOUNT_TOKEN> \
  --docker-email=admin@internal.corp \
  -n odm-pilot
```

## Option B: IBM Registry (Standard Lab)
**Target Environment:** Sandbox / POC with Internet Access

If you are working in a lab with direct internet access and no strict OPA registry constraints, you can pull directly from IBM.

**Action:** Create a secret using your IBM Entitlement Key.

```bash
# 1. Get your key from myibm.ibm.com/products-services/containerlibrary
# 2. Create the secret
kubectl create secret docker-registry internal-registry-secret \
  --docker-server=cp.icr.io \
  --docker-username=cp \
  --docker-password=<YOUR_IBM_ENTITLEMENT_KEY> \
  --docker-email=user@example.com \
  -n odm-pilot
```
:::

:::{.callout-important}
### Secret Name Consistency
Whichever option you choose, ensure the secret name used in the `kubectl create` command exactly matches the value in your `values.yaml` file under `image.pullSecrets`.
:::